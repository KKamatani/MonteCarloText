[["index.html", "練習問題 : モンテカルロ統計計算 Readme 0.1 RStanについて", " 練習問題 : モンテカルロ統計計算 鎌谷 研吾 (Kengo Kamatani, ISM and JST CREST) Readme 本ウェブサイトは講談社の書籍「モンテカルロ統計計算」の練習問題の回答ページである．本サイトの作成にはYuhui Xieの素晴らしいR package，bookdownを用いた． 回答の中にRコードも含むため，あらかじめ以下の設定をし，パッケージを読み込むこと． 部分的に当該書籍の内容を参照しているため，Self-contained とはいえないが，多くの部分は当該書籍なしに読むことに問題はないだろう． knitr::opts_chunk$set(echo = TRUE, cache = TRUE) library(ggplot2) library(reshape2) library(plotly) library(microbenchmark) library(latex2exp) library(rstan) blue &lt;- &quot;#4271AE&quot; black &lt;- &quot;#1F3552&quot; 0.1 RStanについて 本ウェブサイトでは，教科書で扱っていない rstanパッケージを少し使う．事前に設定が必要なので注意してほしい．インストールに際してはrstanのウェブサイトを確認してほしい．手順としては， まず最新版のRをインストール - R Projectのウェブサイト C++を使うため，C++ Toolchainをインストール それぞれのプラットフォームごとに以下のリンクをたどること． Windows - Configuring C++ Toolchain Mac - Configuring C++ Toolchain Linux - Configuring C++ Toolchain なお，Macについて補足すると，ARM Processor（Apple M1）では，いまのところ The Coatless professorのブログ の通りに gfortranをインストールすればよいが gfortran 10.2 for Big Sur (macOS 11), for Intel processors をインストールすること．Intel Processor用だが，Rosetta 2で，ARM Processorでも動く．RcppArmadilloのバージョンを古いものにする必要があるかもしれない． 念の為すでにrstanがあればそれと.RDataを削除し， remove.packages(&quot;rstan&quot;) if (file.exists(&quot;.RData&quot;)) file.remove(&quot;.RData&quot;) また，念の為，リポジトリを指定してインストール Sys.setenv(DOWNLOAD_STATIC_LIBV8 = 1) # only necessary for Linux without the nodejs library / headers install.packages(&quot;rstan&quot;, repos = &quot;https://cloud.r-project.org/&quot;, dependencies = TRUE) インストールが無事にできているか確認のため， example(stan_model, package = &quot;rstan&quot;, run.dontrun = TRUE) を実行すると良い．Warningが出ても気にしなくてよいが，Errorが出たらうまく行っていないということだ． "],["tab-1.html", "Chapter 1 序論", " Chapter 1 序論 Exercise 1.1 \\(\\mathbb{P}(A)&gt;0, \\mathbb{P}(B)&gt;0\\)とする． \\[\\mathbb{P}(A\\mid B)=\\mathbb{P}(B\\mid A)\\] が成り立つのために，\\(\\mathbb{P}(A), \\mathbb{P}(B)\\)および\\(\\mathbb{P}(A\\cap B)\\)が満たすべき条件を求めよ． Solution. 本問はひっかけ問題である．条件付き確率の定義から，もとめる等号が成り立つことと \\[\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)}=\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(A)}\\] が成り立つことは同値である．もし\\(\\mathbb{P}(A\\cap B)&gt;0\\)なら，両辺を\\(\\mathbb{P}(A\\cap B)\\)でわって，\\(\\mathbb{P}(A)=\\mathbb{P}(B)\\)を得る．じつは，これだけがこたえではなく，\\(\\mathbb{P}(A\\cap B)=0\\)なら，\\(\\mathbb{P}(A), \\mathbb{P}(B)\\)の値がなんであれ成り立つ．したがって，求める等号が成り立つのはつぎの２つの場合である．ひとつは \\(\\mathbb{P}(A\\cap B)&gt;0\\)かつ\\(\\mathbb{P}(A)=\\mathbb{P}(B)\\)のとき．もうひとつは\\(\\mathbb{P}(A\\cap B)=0\\)となるときである． Exercise 1.2 事象\\(A, B\\)は独立であり，\\(\\mathbb{P}(B)&gt;0\\)とする．また，\\(\\mathbb{P}(B\\mid A\\cup B)=2/3, \\mathbb{P}(A\\mid B)=1/2\\)とする．このとき\\(\\mathbb{P}(B)\\)を求めよ． Solution. \\(A\\)と\\(B\\)が独立だから， \\[\\mathbb{P}(A\\mid B)=\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)}=\\frac{\\mathbb{P}(A)\\mathbb{P}(B)}{\\mathbb{P}(B)}=\\mathbb{P}(A)\\] となる．したがって\\(\\mathbb{P}(A)=\\mathbb{P}(A^c)=1/2\\)である．いっぽう， \\[\\mathbb{P}(A\\cup B)=\\mathbb{P}(A)+\\mathbb{P}(A^c\\cap B)=\\mathbb{P}(A)+\\mathbb{P}(A^c)\\mathbb{P}(B)=\\frac{1}{2}(1+\\mathbb{P}(B))\\] である．よって \\[\\mathbb{P}(B\\mid A\\cup B)=\\frac{\\mathbb{P}(B)}{\\mathbb{P}(A\\cap B)}=\\frac{\\mathbb{P}(B)}{(1+\\mathbb{P}(B))/2}=\\frac{2}{3}\\] となり，かんたんな計算から\\(\\mathbb{P}(B)=1/2\\)を得る． Exercise 1.3 ある大学では，学生のうち\\(60\\%\\)はスニーカーを履いている．一方，教職員のうち\\(20\\%\\)がスニーカーを履いている．大学の構成員のうち，\\(60\\%\\)が学生，\\(40\\%\\)が教職員とする．その大学の，ある構成員はスニーカーを履いていたとする．このとき彼もしくは彼女が学生である確率を求めよ． Solution. （20200312解答修正） 大学の構成員を勝手に一人選んだとき，それが学生である事象を\\(A\\)，スニーカーを履いている事象を\\(B\\)とする．すると \\[\\mathbb{P}(A)=0.6, \\mathbb{P}(B\\mid A)=0.6, \\mathbb{P}(B\\mid A^c)=0.2\\] である．また，求める確率は\\(\\mathbb{P}(A\\mid B)\\)である．条件付き確率の定義から \\[\\begin{align*} \\mathbb{P}(A\\mid B)&amp;=\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)}\\\\ &amp;=\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(A\\cap B)+\\mathbb{P}(A^c\\cap B)}\\\\ &amp;=\\frac{\\mathbb{P}(B\\mid A)\\mathbb{P}(A)}{\\mathbb{P}(B\\mid A)\\mathbb{P}(A)+\\mathbb{P}(B\\mid A^c)\\mathbb{P}(A^c)}\\\\ &amp;=\\frac{0.6\\times 0.6}{0.6\\times 0.6+0.2\\times 0.4}=\\frac{9}{11}. \\end{align*}\\] Exercise 1.4 \\(N\\)は正の整数，\\(\\sigma^2\\)は正の実数．観測\\(x_1,\\ldots, x_N\\)は独立で\\(\\mathcal{N}(\\theta,\\sigma^2)\\)に従い，\\(\\theta\\)の事前分布を\\(\\mathcal{N}(0,\\sigma^2)\\)とする．このとき\\(\\theta\\)の事後分布を求めよ． Solution. 観測をまとめて\\(x^N\\)と書くと，尤度は \\[\\begin{align*} p(x^N\\mid \\theta)&amp;=\\prod_{n=1}^N\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(x_n-\\theta)^2}{2\\sigma^2}\\right)\\\\ &amp;\\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{n=1}^N(x_n-\\theta)^2\\right) \\end{align*}\\] となる．従って，事後確率密度関数は \\[\\begin{align*} p(\\theta\\mid x^N)&amp;\\propto p(x^N\\mid \\theta)p(\\theta)\\\\ &amp;\\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{n=1}^N(x_n-\\theta)^2-\\frac{\\theta^2}{2\\sigma^2}\\right) \\end{align*}\\] となる．指数の中身を平方完成すると \\[\\begin{align*} -\\frac{N+1}{2\\sigma^2}\\left(\\theta-\\frac{1}{N+1}\\sum_{n=1}^N x_n\\right)^2+C \\end{align*}\\] となる．ただし，実数\\(C\\)は\\(\\theta\\)によらない定数とする．よって事後密度関数の形から，事後分布は \\(\\mathcal{N}\\left(\\sum_{n=1}^Nx_n/(N+1), \\sigma^2/(N+1)\\right)\\). rstanを用いて事後分布を描画してみよう．rstanはベイズ統計学の応用でしばしば使われるStan言語のR版である．教科書では扱えなかったが，ちょっとした計算に便利だ．なお，rstanでは常に，事後分布を導出する際には乱数を用いた近似が利用されることに注意せよ．まずrstanのために，モデルを記述しよう．こうしたことはたいてい，固い教科書よりもユーザによるウェブサイトが役に立つ．説明はそちらに譲る． モデルの記述の仕方はいくつかあるが，ここでは下記のモデルをnormal.stanというチャンクに記述する． data { int N; real x[N]; real&lt;lower=0&gt; sigma; } parameters { real theta; } model { theta ~ normal(0, sigma); // 事前分布 x ~ normal(theta, sigma); // 尤度 } そしてそのモデルをrstanの関数に代入しよう．最初の二行はおまじないなので気にしなくて良い． rstan_options(auto_write = TRUE) options(mc.cores = parallel::detectCores()) # データの生成 sigma &lt;- 1.0 x &lt;- rnorm(10,1,sigma) normal_dat &lt;- list(N = length(x), x = x, sigma = sigma) # コンパイル fit &lt;- rstan::sampling(normal.stan, data = normal_dat, iter = 10^4, chains = 4) # stanからparameterの取り出し theta &lt;- extract(fit, &#39;theta&#39;) theta &lt;- unlist(theta, use.names=FALSE) # 理論的な事後密度関数 t &lt;- mean(x)+seq(-1,1,length=1000) fit &lt;- dnorm(t, sum(x)/(length(x)+1), sigma/sqrt(length(x)+1)) fig &lt;- plot_ly(x = theta, type=&quot;histogram&quot;, histnorm = &quot;probability density&quot;, name=&quot;rstan&quot;) fig %&gt;% add_lines(x = t, y = fit, mode = &#39;lines&#39;, name=&quot;exact&quot;) おおよそ上で計算した理論的な事後密度関数と，rstanで乱数を用いて計算したものは一致するようだ． Exercise 1.5 \\(\\nu,\\alpha\\)が正の実数であるとき，式(1.7)をもちいて，\\(\\mathcal{G}(\\nu,\\alpha)\\)の平均を求めよ． Solution. 式(1.7)より \\[\\begin{align*} \\int_0^\\infty x~\\frac{\\alpha^\\nu x^{\\nu-1}e^{-\\alpha x}}{\\Gamma(\\nu)}\\mathrm{d}x &amp;=\\frac{\\alpha^\\nu}{\\Gamma(\\nu)}\\int_0^\\infty x^\\nu e^{-\\alpha x}\\mathrm{d}x\\\\ &amp;=\\frac{\\alpha^\\nu}{\\Gamma(\\nu)}\\frac{\\Gamma(\\nu+1)}{\\alpha^{\\nu+1}}\\\\ &amp;=\\frac{\\nu}{\\alpha}. \\end{align*}\\] ただし，\\(\\Gamma(\\nu+1)=\\nu~\\Gamma(\\nu)\\)を用いた． Remark. 記号\\(\\propto\\)を使うのは，事後分布を計算するために不要な計算を回避する，計算のテクニックである．計算をこのように適宜省かないと，事後分布の計算はとても煩雑になりがちだからだ． しかし，初学者は記号\\(\\propto\\)の意味するものが well-defined ではないと感じることだろう．そのため，初学者は教科書に最初に紹介した，記号\\(\\propto\\)をもちいずきっちり事後分布の確率密度関数を計算する方法を何度か経験したほうが良い．その経験のあとなら記号\\(\\propto\\)の便利さと意味を納得できるだろう． Exercise 1.6 \\(N\\)は正の整数，\\(\\mu\\)は実数，\\(\\alpha,\\beta,\\tau\\)は正の実数とする．観測\\(x_1,\\ldots, x_N\\)は\\(\\mathcal{N}(\\mu,\\tau^{-1})\\)に従い独立とする．また，\\(\\tau\\sim\\mathcal{G}(\\alpha,\\beta)\\)とする．実数\\(\\alpha,\\beta,\\mu\\)は既知とするとき，パラメータ\\(\\tau\\)の事後分布を求めよ． Solution. 尤度は \\[\\begin{align*} p(x^N\\mid \\tau)&amp;=\\prod_{n=1}^N\\sqrt{\\frac{\\tau}{2\\pi}}\\exp\\left(-\\frac{(x_n-\\mu)^2}{2}\\tau\\right)\\\\ &amp;\\propto \\tau^{N/2}\\exp\\left(-\\sum_{n=1}^N\\frac{(x_n-\\mu)^2}{2}\\tau\\right) \\end{align*}\\] となる．よって事後密度関数は \\[\\begin{align*} p(\\tau\\mid x^N)\\propto \\tau^{\\alpha+N/2-1}\\exp\\left(-\\left\\{\\beta+\\sum_{n=1}^N\\frac{(x_n-\\mu)^2}{2}\\right\\}\\tau\\right) \\end{align*}\\] となる．よって事後密度関数の形から，事後分布は \\[\\mathcal{G}\\left(\\alpha+\\frac{N}{2}, \\beta+\\sum_{n=1}^N\\frac{(x_n-\\mu)^2}{2}\\right). \\] Remark. 確率分布を求めよ，という問題は，求めるべき確率分布がよく知られた確率分布になることを暗に意味している．その確率分布が正規分布やガンマ分布など，どの確率分布なのか，そしてパラメータの値がなにかということが聞かれているのである． これもrstanを使って確認しよう．次のモデルをnormal_gamma.stanというチャンクに記述した． data { int N; real x[N]; real&lt;lower=0&gt; alpha; real&lt;lower=0&gt; beta; real mu; } parameters { real tau; } model { tau ~ gamma(alpha, beta); // 事前分布 x ~ normal(mu, 1/sqrt(tau)); // 尤度 } rstanの結果と理論の結果は整合性があるようだ． # データの生成 alpha &lt;- 2.0 beta &lt;- 3.0 mu &lt;- 2.5 tau &lt;- 1.0 x &lt;- rnorm(10,mu,1/sqrt(tau)) normal_gamma_dat &lt;- list(N = length(x), x = x, alpha = alpha, beta = beta, mu = mu) # コンパイル fit &lt;- rstan::sampling(normal_gamma.stan, data = normal_gamma_dat, iter = 10^4, chains = 4) ## ## SAMPLING FOR MODEL &#39;4158cf09bd62d921e268909ea92a86fb&#39; NOW (CHAIN 1). ## Chain 1: Rejecting initial value: ## Chain 1: Error evaluating the log probability at the initial value. ## Chain 1: Exception: normal_lpdf: Scale parameter is nan, but must be &gt; 0! (in &#39;modela68872b8d8a9_4158cf09bd62d921e268909ea92a86fb&#39; at line 13) ## ## Chain 1: Rejecting initial value: ## Chain 1: Error evaluating the log probability at the initial value. ## Chain 1: Exception: normal_lpdf: Scale parameter is nan, but must be &gt; 0! (in &#39;modela68872b8d8a9_4158cf09bd62d921e268909ea92a86fb&#39; at line 13) ## ## Chain 1: ## Chain 1: Gradient evaluation took 1.8e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 10000 [ 0%] (Warmup) ## Chain 1: Iteration: 1000 / 10000 [ 10%] (Warmup) ## Chain 1: Iteration: 2000 / 10000 [ 20%] (Warmup) ## Chain 1: Iteration: 3000 / 10000 [ 30%] (Warmup) ## Chain 1: Iteration: 4000 / 10000 [ 40%] (Warmup) ## Chain 1: Iteration: 5000 / 10000 [ 50%] (Warmup) ## Chain 1: Iteration: 5001 / 10000 [ 50%] (Sampling) ## Chain 1: Iteration: 6000 / 10000 [ 60%] (Sampling) ## Chain 1: Iteration: 7000 / 10000 [ 70%] (Sampling) ## Chain 1: Iteration: 8000 / 10000 [ 80%] (Sampling) ## Chain 1: Iteration: 9000 / 10000 [ 90%] (Sampling) ## Chain 1: Iteration: 10000 / 10000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.084998 seconds (Warm-up) ## Chain 1: 0.062902 seconds (Sampling) ## Chain 1: 0.1479 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;4158cf09bd62d921e268909ea92a86fb&#39; NOW (CHAIN 2). ## Chain 2: Rejecting initial value: ## Chain 2: Error evaluating the log probability at the initial value. ## Chain 2: Exception: normal_lpdf: Scale parameter is nan, but must be &gt; 0! (in &#39;modela68872b8d8a9_4158cf09bd62d921e268909ea92a86fb&#39; at line 13) ## ## Chain 2: Rejecting initial value: ## Chain 2: Error evaluating the log probability at the initial value. ## Chain 2: Exception: normal_lpdf: Scale parameter is nan, but must be &gt; 0! (in &#39;modela68872b8d8a9_4158cf09bd62d921e268909ea92a86fb&#39; at line 13) ## ## Chain 2: ## Chain 2: Gradient evaluation took 4e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 10000 [ 0%] (Warmup) ## Chain 2: Iteration: 1000 / 10000 [ 10%] (Warmup) ## Chain 2: Iteration: 2000 / 10000 [ 20%] (Warmup) ## Chain 2: Iteration: 3000 / 10000 [ 30%] (Warmup) ## Chain 2: Iteration: 4000 / 10000 [ 40%] (Warmup) ## Chain 2: Iteration: 5000 / 10000 [ 50%] (Warmup) ## Chain 2: Iteration: 5001 / 10000 [ 50%] (Sampling) ## Chain 2: Iteration: 6000 / 10000 [ 60%] (Sampling) ## Chain 2: Iteration: 7000 / 10000 [ 70%] (Sampling) ## Chain 2: Iteration: 8000 / 10000 [ 80%] (Sampling) ## Chain 2: Iteration: 9000 / 10000 [ 90%] (Sampling) ## Chain 2: Iteration: 10000 / 10000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.082411 seconds (Warm-up) ## Chain 2: 0.062013 seconds (Sampling) ## Chain 2: 0.144424 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;4158cf09bd62d921e268909ea92a86fb&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 5e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 10000 [ 0%] (Warmup) ## Chain 3: Iteration: 1000 / 10000 [ 10%] (Warmup) ## Chain 3: Iteration: 2000 / 10000 [ 20%] (Warmup) ## Chain 3: Iteration: 3000 / 10000 [ 30%] (Warmup) ## Chain 3: Iteration: 4000 / 10000 [ 40%] (Warmup) ## Chain 3: Iteration: 5000 / 10000 [ 50%] (Warmup) ## Chain 3: Iteration: 5001 / 10000 [ 50%] (Sampling) ## Chain 3: Iteration: 6000 / 10000 [ 60%] (Sampling) ## Chain 3: Iteration: 7000 / 10000 [ 70%] (Sampling) ## Chain 3: Iteration: 8000 / 10000 [ 80%] (Sampling) ## Chain 3: Iteration: 9000 / 10000 [ 90%] (Sampling) ## Chain 3: Iteration: 10000 / 10000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.079113 seconds (Warm-up) ## Chain 3: 0.072525 seconds (Sampling) ## Chain 3: 0.151638 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;4158cf09bd62d921e268909ea92a86fb&#39; NOW (CHAIN 4). ## Chain 4: Rejecting initial value: ## Chain 4: Error evaluating the log probability at the initial value. ## Chain 4: Exception: normal_lpdf: Scale parameter is nan, but must be &gt; 0! (in &#39;modela68872b8d8a9_4158cf09bd62d921e268909ea92a86fb&#39; at line 13) ## ## Chain 4: Rejecting initial value: ## Chain 4: Error evaluating the log probability at the initial value. ## Chain 4: Exception: normal_lpdf: Scale parameter is nan, but must be &gt; 0! (in &#39;modela68872b8d8a9_4158cf09bd62d921e268909ea92a86fb&#39; at line 13) ## ## Chain 4: ## Chain 4: Gradient evaluation took 8e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 10000 [ 0%] (Warmup) ## Chain 4: Iteration: 1000 / 10000 [ 10%] (Warmup) ## Chain 4: Iteration: 2000 / 10000 [ 20%] (Warmup) ## Chain 4: Iteration: 3000 / 10000 [ 30%] (Warmup) ## Chain 4: Iteration: 4000 / 10000 [ 40%] (Warmup) ## Chain 4: Iteration: 5000 / 10000 [ 50%] (Warmup) ## Chain 4: Iteration: 5001 / 10000 [ 50%] (Sampling) ## Chain 4: Iteration: 6000 / 10000 [ 60%] (Sampling) ## Chain 4: Iteration: 7000 / 10000 [ 70%] (Sampling) ## Chain 4: Iteration: 8000 / 10000 [ 80%] (Sampling) ## Chain 4: Iteration: 9000 / 10000 [ 90%] (Sampling) ## Chain 4: Iteration: 10000 / 10000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.081844 seconds (Warm-up) ## Chain 4: 0.06383 seconds (Sampling) ## Chain 4: 0.145674 seconds (Total) ## Chain 4: # stanからparameterの取り出し tau &lt;- extract(fit, &#39;tau&#39;) tau &lt;- unlist(tau, use.names=FALSE) # 理論的な事後密度関数 t &lt;- seq(0,4,length=1000) fit &lt;- dgamma(t, shape = alpha + length(x)/2, rate = beta + sum((x-mu)^2)/2) fig &lt;- plot_ly(x = tau, type=&quot;histogram&quot;, histnorm = &quot;probability density&quot;, name=&quot;rstan&quot;) fig %&gt;% add_lines(x = t, y = fit, mode = &#39;lines&#39;, name=&quot;exact&quot;) Exercise 1.7 \\(\\alpha, \\beta\\)は正の実数とする．観測\\(x\\)は\\(\\mathcal{E}(\\theta)\\)に従い，\\(\\theta\\)の事前分布を\\(\\mathcal{G}(\\alpha,\\beta)\\)とする． (a). このとき\\(\\theta\\)の事後分布を求めよ． (b). 事後平均を求めよ． (c). 事後予測分布を求めよ． Solution. (a). 事後密度関数は \\[\\begin{align*} p(\\theta\\mid x)&amp;\\propto \\theta e^{-\\theta x} \\theta^{\\alpha-1}e^{-\\beta\\theta}\\\\ &amp;\\propto \\theta^\\alpha e^{-(x+\\beta)\\theta} \\end{align*}\\] となる．従って，事後密度関数の形から，事後分布は \\(\\mathcal{G}(\\alpha+1, x+\\beta)\\). (b). 練習問題1.5より \\[\\frac{\\alpha+1}{x+\\beta}.\\] (c). 正の実数\\(x^*\\)を将来の観測とすると，事後予測分布の確率密度関数は \\[\\begin{align*} p(x^*\\mid x)&amp;=\\int_0^\\infty\\theta e^{-\\theta x^*}\\frac{(x+\\beta)^{\\alpha+1}\\theta^\\alpha}{\\Gamma(\\alpha+1)} e^{-(x+\\beta)\\theta}\\mathrm{d}\\theta\\\\ &amp;=\\int_0^\\infty\\frac{(x+\\beta)^{\\alpha+1}\\theta^{\\alpha+1}}{\\Gamma(\\alpha+1)} e^{-(x+x^*\\beta)\\theta}\\mathrm{d}\\theta\\\\ &amp;=\\frac{\\Gamma(\\alpha+2)}{(x+x^*+\\beta)^{\\alpha+2}}\\frac{(x+\\beta)^{\\alpha+1}}{\\Gamma(\\alpha+1)}\\\\ &amp;=\\frac{(\\alpha+1)(x+\\beta)^{\\alpha+1}}{(x+x^*+\\beta)^{\\alpha+2}}. \\end{align*}\\] Exercise 1.8 観測\\(x\\)は\\(\\mathcal{N}(\\theta_1,\\theta_2^{-1})\\)に従い，\\(\\theta_1\\)の事前分布を\\(\\mathcal{N}(0,\\theta_2^{-1})\\)とし，さらに\\(\\theta_2\\)の事前分布を\\(\\mathcal{E}(1)\\)とする． (a). \\(\\theta_1\\)と\\(x\\)で条件づけた\\(\\theta_2\\)の条件つき分布を求めよ． (b). \\(\\theta_1\\)の周辺事後分布を求めよ． Solution. （20200312解答修正） (a). 尤度は \\[p(x\\mid \\theta_1,\\theta_2)=\\sqrt{\\frac{\\theta_2}{2\\pi}}\\exp\\left(-\\frac{(x-\\theta_1)^2}{2}\\theta_2\\right)\\] と書ける．したがって事後分布の確率密度関数は \\[\\begin{align*} p(\\theta_1,\\theta_2\\mid x) &amp;\\propto p(x\\mid \\theta_1,\\theta_2)p(\\theta_1\\mid \\theta_2)p(\\theta_2)\\\\ &amp;= \\sqrt{\\frac{\\theta_2}{2\\pi}}\\exp\\left(-\\frac{(x-\\theta_1)^2}{2}\\theta_2\\right)\\sqrt{\\frac{\\theta_2}{2\\pi}}\\exp\\left(-\\frac{\\theta_1^2}{2}\\theta_2\\right)\\exp(-\\theta_2)\\\\ &amp;\\propto \\theta_2\\exp\\left(-\\left(\\frac{(x-\\theta_1)^2}{2}+\\frac{\\theta_1^2}{2}+1\\right)\\theta_2\\right) \\end{align*}\\] となる．すると\\(p(\\theta_2\\mid x, \\theta_1,)\\propto p(\\theta_1,\\theta_2\\mid x)\\)だから\\(\\theta_1\\)と\\(x\\)で条件づけた\\(\\theta_2\\)の条件つき分布は ガンマ分布とパラメータを見比べると， \\[\\mathcal{G}_a\\left(2,\\frac{(x-\\theta_1)^2}{2}+\\frac{\\theta_1^2}{2}+1\\right)\\] であることがわかる． (b). ガンマ関数と見比べることで， \\[\\begin{align*} p(\\theta_1\\mid x)&amp;=\\int_0^\\infty p(\\theta_1,\\theta_2\\mid x)\\mathrm{d}\\theta_2\\\\ &amp;=\\left(\\frac{(x-\\theta_1)^2}{2}+\\frac{\\theta_1^2}{2}+1\\right)^{-2}\\int_0^\\infty \\left(\\frac{(x-\\theta_1)^2}{2}+\\frac{\\theta_1^2}{2}+1\\right)^2\\\\ &amp;\\quad\\theta_2\\exp\\left(-\\left(\\frac{(x-\\theta_1)^2}{2}+\\frac{\\theta_1^2}{2}+1\\right)\\theta_2\\right)\\mathrm{d}\\theta_2\\\\ &amp;=\\left(\\frac{(x-\\theta_1)^2}{2}+\\frac{\\theta_1^2}{2}+1\\right)^{-2}\\Gamma(2) \\end{align*}\\] となる．だから\\(t\\)分布のパラメータ（16ページ）と見比べれば，これは \\[ \\mathcal{T}_3\\left(\\frac{x}{2}, \\left\\{\\frac{1}{3}\\left(1+\\frac{x^2}{4}\\right)\\right\\}^{1/2}\\right) \\] の確率密度関数である．したがって，周辺事後分布は上記の\\(t\\)分布である． Remark. ここでは \\[p(\\theta_2\\mid x,\\theta_1)\\propto p(\\theta_1,\\theta_2\\mid x) \\] を用いた．なぜなら， \\[p(\\theta_2\\mid x,\\theta_1)= \\frac{p(\\theta_1,\\theta_2\\mid x)}{p(\\theta_1\\mid x)}\\] であり，\\(\\theta_1\\)と\\(x\\)で条件つけた分布を計算する際には左辺の分母を定数と見て良いからだ．この考え方は，あとでギブスサンプリングを構成する際に重要である． これもrstanを使って確認しよう．今までとは違う方法でrstanを走らせてみよう． モデルをすべてexcodeという変数に代入する． excode &lt;- &#39; data { real x; } parameters { real theta1; real&lt;lower=0&gt; theta2; } model { theta1 ~ normal(0, 1/sqrt(theta2)); // 事前分布 theta2 ~ exponential(1); // 事前分布 x ~ normal(theta1, 1/sqrt(theta2)); // 尤度 }&#39; コンパイルする際も先ほどと命令が若干異なる． # データの生成 theta1 &lt;- 2.0 theta2 &lt;- 1.0 x &lt;- rnorm(1,theta1,1/sqrt(theta2)) normal_exp_dat &lt;- list(x = x) # コンパイル fit &lt;- stan(model_code = excode, data = normal_exp_dat, iter = 10000, chains = 4) # stanからparameterの取り出し theta1 &lt;- extract(fit, &#39;theta1&#39;) theta1 &lt;- unlist(theta1, use.names=FALSE) # 理論的な事後密度関数 t &lt;- x/2 + seq(-10,10,length=1000) scale &lt;- sqrt((1 + x^2/4)/3) fit &lt;- dt( (t - x/2)/scale, df = 3)/scale fig &lt;- plot_ly(x = theta1, type=&quot;histogram&quot;, histnorm = &quot;probability density&quot;, name=&quot;rstan&quot;) fig &lt;- fig %&gt;% add_lines(x = t, y = fit, mode = &#39;lines&#39;, name=&quot;exact&quot;) fig &lt;- fig %&gt;% layout(xaxis = list(range = c(x/2-10, x/2+10))) fig ## Warning: &#39;scatter&#39; objects don&#39;t have these attributes: &#39;histnorm&#39; ## Valid attributes include: ## &#39;cliponaxis&#39;, &#39;connectgaps&#39;, &#39;customdata&#39;, &#39;customdatasrc&#39;, &#39;dx&#39;, &#39;dy&#39;, &#39;error_x&#39;, &#39;error_y&#39;, &#39;fill&#39;, &#39;fillcolor&#39;, &#39;groupnorm&#39;, &#39;hoverinfo&#39;, &#39;hoverinfosrc&#39;, &#39;hoverlabel&#39;, &#39;hoveron&#39;, &#39;hovertemplate&#39;, &#39;hovertemplatesrc&#39;, &#39;hovertext&#39;, &#39;hovertextsrc&#39;, &#39;ids&#39;, &#39;idssrc&#39;, &#39;legendgroup&#39;, &#39;legendgrouptitle&#39;, &#39;legendrank&#39;, &#39;line&#39;, &#39;marker&#39;, &#39;meta&#39;, &#39;metasrc&#39;, &#39;mode&#39;, &#39;name&#39;, &#39;opacity&#39;, &#39;orientation&#39;, &#39;selected&#39;, &#39;selectedpoints&#39;, &#39;showlegend&#39;, &#39;stackgaps&#39;, &#39;stackgroup&#39;, &#39;stream&#39;, &#39;text&#39;, &#39;textfont&#39;, &#39;textposition&#39;, &#39;textpositionsrc&#39;, &#39;textsrc&#39;, &#39;texttemplate&#39;, &#39;texttemplatesrc&#39;, &#39;transforms&#39;, &#39;type&#39;, &#39;uid&#39;, &#39;uirevision&#39;, &#39;unselected&#39;, &#39;visible&#39;, &#39;x&#39;, &#39;x0&#39;, &#39;xaxis&#39;, &#39;xcalendar&#39;, &#39;xhoverformat&#39;, &#39;xperiod&#39;, &#39;xperiod0&#39;, &#39;xperiodalignment&#39;, &#39;xsrc&#39;, &#39;y&#39;, &#39;y0&#39;, &#39;yaxis&#39;, &#39;ycalendar&#39;, &#39;yhoverformat&#39;, &#39;yperiod&#39;, &#39;yperiod0&#39;, &#39;yperiodalignment&#39;, &#39;ysrc&#39;, &#39;key&#39;, &#39;set&#39;, &#39;frame&#39;, &#39;transforms&#39;, &#39;_isNestedKey&#39;, &#39;_isSimpleKey&#39;, &#39;_isGraticule&#39;, &#39;_bbox&#39; Exercise 1.9 本問は例1.8の二項分布から多項分布への拡張である． \\(K\\)は正の整数，\\(\\alpha_1,\\ldots,\\alpha_K\\)は正の実数とする． \\(\\mathcal{D}(\\alpha_1,\\ldots,\\alpha_K)\\)は確率密度関数 \\[p(\\theta_1,\\ldots,\\theta_{K-1})= \\left\\{ \\begin{array}{ll} \\frac{\\Gamma(\\alpha_1+\\cdots+\\alpha_K)}{\\Gamma(\\alpha_1)\\cdots\\Gamma(\\alpha_K)}\\theta_1^{\\alpha_1-1}\\cdots\\theta_K^{\\alpha_K-1}&amp;\\mathrm{if}\\ 0&lt;\\theta_k&lt;1, k=1,\\ldots, K\\\\ 0&amp;\\mathrm{otherwise} \\end{array} \\right.\\] をもつ．ただし，\\(\\theta=(\\theta_1,\\theta_2,\\ldots,\\theta_{K-1})\\)とし， \\[\\theta_K=\\theta_K(\\theta)=1-\\theta_1-\\cdots-\\theta_{K-1}\\] とする．さらに，\\(N\\)は正の整数で，\\(\\theta_1,\\ldots, \\theta_{K-1}\\)は正の実数で\\(\\theta_1+\\cdots+\\theta_{K-1}&lt;1\\)のとき，) \\(\\mathcal{M}(N,\\theta_1,\\ldots,\\theta_{K-1})\\)は確率関数 \\[p(x_1,\\ldots, x_{K-1})= \\left\\{ \\begin{array}{ll} \\frac{N!}{x_1!\\cdots x_K!}\\theta_1^{x_1}\\cdots \\theta_K^{x_K} &amp;\\mathrm{if}\\ 0\\le x_k\\le N, k=1,\\ldots, K\\\\ 0&amp;\\mathrm{otherwise} \\end{array} \\right.\\] をもつ．ただし，\\(x=(x_1,\\ldots, x_{K-1})\\)は正の整数で \\[x_K=x_K(x)=N-x_1-\\cdots-x_{K-1}\\] とする． \\(x\\mid \\theta\\sim \\mathcal{M}(N,\\theta)\\), \\(\\theta\\sim \\mathcal{D}(\\alpha_1,\\ldots,\\alpha_K)\\)であるとき，\\(\\theta\\)の事後分布をもとめよ． Solution. 尤度が \\[p(x\\mid \\theta)\\propto \\theta_1^{x_1}\\cdots \\theta_K^{x_K}\\] と書けるから事後分布の確率密度関数は \\[\\begin{align*} p(\\theta\\mid x)&amp;\\propto p(x\\mid \\theta)p(\\theta)\\\\ &amp;\\propto \\theta_1^{x_1}\\cdots \\theta_K^{x_K}~\\theta_1^{\\alpha_1-1}\\cdots\\theta_K^{\\alpha_K-1}\\\\ &amp;= \\theta_1^{\\alpha_1+x_1-1}\\cdots \\theta_K^{\\alpha_K+x_K-1} \\end{align*}\\] となる．だから，ディリクレ分布の確率密度関数と見比べれば，事後分布が \\(\\mathcal{D}(\\alpha_1+x_1,\\ldots, \\alpha_K+x_K)\\) であることがわかる． Remark. ディリクレ分布は和が\\(1\\)になる正の数列\\(\\theta_1,\\ldots, \\theta_K\\)に値を取る確率分布である．とくに\\(K=2\\)としたときがベータ分布である．和をとると\\(1\\)になる列は多項分布の確率関数になるから，ディリクレ分布は確率分布の確率分布とも捉えられる．うえの練習問題はこの事実を使った． ベータ分布は\\(K=2\\)であり，\\(\\theta_1+\\theta_2=1\\)となるので，\\(\\theta_1\\)さえわかれば\\(\\theta_2\\)もわかるので確率密度関数を描画するのもたやすい．だから，ベータ分布のパラメータがベータ分布にどのように影響するかをみることも容易である．それに比べると，\\(K\\ge 3\\)のディリクレ分布の形状の把握は難しい．ここでは\\(K=3\\)の場合を考えてみよう．じつは \\[X_1\\sim\\mathcal{G}(\\alpha_1,1),\\ldots, X_K\\sim\\mathcal{G}(\\alpha_K,1)\\] であり，独立であるとき， \\[\\theta=(\\theta_1,\\ldots,\\theta_K)=\\left(\\frac{X_1}{\\sum_{k=1}^KX_k},\\ldots, \\frac{X_K}{\\sum_{k=1}^KX_k}\\right)\\] とすると，\\((\\theta_1,\\ldots,\\theta_{K-1})\\)は\\(\\mathcal{D}(\\alpha_1,\\ldots,\\alpha_K)\\)に従う．以下では \\(K=3\\)のときに，乱数\\(\\theta\\)をたくさん生成し，３次元空間に描画した．パラメータを変えて実験してみると良い． set.seed(1234) N &lt;- 1e3 alpha &lt;- c(1.0, 1.5, 1.8) #Parameter x &lt;- rgamma(N, alpha[1]) y &lt;- rgamma(N, alpha[2]) z &lt;- rgamma(N, alpha[3]) w &lt;- x + y + z data.fr &lt;- data.frame(x=x/w, y=y/w, z=z/w) plot_ly(data.fr, x=~x, y=~y, z=~z) %&gt;% add_markers(marker=list(size=3)) Exercise 1.10 例1.14を参考に，実数\\(\\mu\\), 正の実数\\(\\sigma\\)による正規分布\\(\\mathcal{N}(\\mu,\\sigma^2)\\)に対するジェフリーズ事前分布を計算したい．パラメータを\\(\\theta=(\\mu,\\sigma^2)\\)としたときのジェフリーズ事前分布をフィッシャー情報行列の計算による方法と，不変性を用いた方法の二通りの方法で求めよ． Solution. \\(\\xi=\\sigma^2\\)とおく．すると正規分布\\(\\mathcal{N}(\\mu,\\xi)\\)の確率密度関数は \\[\\sqrt{\\frac{1}{2\\pi\\xi}}\\exp\\left(-\\frac{1}{2\\xi}(x-\\mu)^2\\right)\\] である．だからスコア関数は \\[\\begin{matrix}S_1\\\\S_2\\end{matrix}:=\\begin{pmatrix}\\xi^{-1}(x-\\mu)\\\\-\\frac{1}{2\\xi}+\\xi^{-2}\\frac{(x-\\mu)^2}{2}\\end{pmatrix}\\] となる．いっぽう，例1.14で紹介したように， \\[\\mathbb{E}[x-\\mu]=\\mathbb{E}[(x-\\mu)^3]=0,\\ \\mathbb{E}[(x-\\mu)^2]=\\xi,\\ \\mathbb{E}[(x-\\mu)^4]=3\\xi^2\\] となる．したがって，フィッシャー情報行列は \\[\\mathbb{E}\\left[\\begin{pmatrix}S_1^2&amp;S_1S_2\\\\S_2S_1&amp;S_2^2\\end{pmatrix}\\right]=\\begin{pmatrix}\\xi^{-1}&amp;0\\\\0&amp;\\xi^{-2}/2\\end{pmatrix}\\] である．この行列の行列式は\\(\\xi^{-3}/2\\)となるから，パラメータを\\((\\mu,\\xi)\\)としたときのジェフリーズ事前分布は \\[2^{-1/2}\\xi^{-3/2}\\] を密度関数として持つ．これでひとつめの直接的な方法でのジェフリーズ事前分布を導出できた． いっぽう，例1.14では\\(\\tau=\\xi^{-1}\\)としてパラメータを\\((\\mu,\\xi)\\)としたときのジェフリーズ事前分布が \\[\\tau^{-1/2}\\] を密度関数として持つことがわかっている．また \\[(\\mu,\\tau)\\mapsto (\\mu,\\xi)\\] という変数変換によるヤコビ行列式は \\[\\left|\\det\\begin{pmatrix}1&amp;0\\\\0&amp;-\\xi^{-2}\\end{pmatrix}\\right|=\\xi^{-2}\\] である．よってジェフリーズ事前分布の変数変換による不変性から，パラメータを\\((\\mu,\\xi)\\)としたときのジェフリーズ事前分布は \\[\\xi^{1/2}~\\xi^{-2}=\\xi^{-3/2}\\] となり，たしかにさきほど求めたものと一致する （注: 次のRemark を見よ）． Remark. この例のジェフリーズ事前分布のように非正則な分布に対しては，密度関数の定数倍の差は無視して良い．このことを，非正則な分布は定数倍の任意性があるとも言う． Exercise 1.11 \\(N\\)は正の整数，\\(\\lambda_1,\\lambda_2\\)は正の実数とする． \\(x_1^1,\\ldots, x_N^1\\mid \\lambda_1\\sim\\mathcal{P}(\\lambda_1)\\)， \\(x_1^2,\\ldots, x_N^2\\mid \\lambda_2\\sim\\mathcal{P}(\\lambda_2)\\)であり すべて独立とする．二つのモデル \\[\\mathcal{M}_1:\\lambda_1,\\lambda_2\\sim \\mathcal{E}(1), \\mathcal{M}_0:\\lambda_1=\\lambda_2\\sim\\mathcal{E}(1)\\] を考える．二つのモデルへの事前確率は\\(0.5\\)ずつとする． (a). モデル\\(\\mathcal{M}_0\\)の事後確率を求めよ． (b). ベイズ因子\\(B_{10}\\)を求めよ． Solution. 例1.15でつかった\\(x^N\\)や\\(x_1^N, x_2^N\\)といった記号を再利用したうえで，さらに記号をいくつか用意しよう．各集団の和をそれぞれ \\[S_1=\\sum_{n=1}^Nx_n^1,\\ S_2=\\sum_{n=1}^Nx_n^2\\] とし，全体の和を \\[S=S_1+S_2\\] とする．また，各集団の積をそれぞれ \\[T_1=x_1^1\\cdots x_N^1,\\ T_2=x_1^2\\cdots x_N^2\\] とし，全体の積を \\[T=T_1+T_2\\] としよう．すると，モデル\\(\\mathcal{M}_1\\)のもとの周辺事後確率密度関数は \\[p(x^N\\mid 1)=\\int_0^\\infty p(x_1^N\\mid \\lambda_1)p(\\lambda_1)\\mathrm{d}\\lambda_1\\times \\int_0^\\infty p(x_2^N\\mid \\lambda_2)p(\\lambda_2)\\mathrm{d}\\lambda_2\\] となる．うえの２つの積分の積のうち，最初の一つは \\[\\int_0^\\infty\\frac{\\lambda_1^{S_1}}{T_1}e^{-N\\lambda_1}~e^{-\\lambda_1}\\mathrm{d}\\lambda_1=\\frac{\\Gamma(1+S_1)}{T_1(N+1)^{1+S_1}}\\] と計算できる．２つ目の積についても同じ計算をすれば \\[\\int_0^\\infty\\frac{\\lambda_2^{S_2}}{T_2}e^{-N\\lambda_2}~e^{-\\lambda_2}\\mathrm{d}\\lambda_2=\\frac{\\Gamma(1+S_2)}{T_2(N+1)^{1+S_2}}\\] となることがわかる．よってモデル\\(\\mathcal{M}_1\\)のもとの周辺事後確率密度関数は \\[p(x^N\\mid 1)=\\frac{\\Gamma(1+S_1)\\Gamma(1+S_2)}{T(N+1)^{2+S}}\\] となる．まったく同じ計算で，モデル\\(\\mathcal{M}_0\\)の場合は \\[p(x^N\\mid 0)=\\int_0^\\infty\\frac{\\lambda_1^{S}}{T}e^{-2N\\lambda}~e^{-\\lambda}\\mathrm{d}\\lambda=\\frac{\\Gamma(1+S)}{T(2N+1)^{1+S}}\\] を得る．したがって \\[\\frac{p(x^N\\mid 0)}{p(x^N\\mid 1)}=\\frac{(N+1)^{2+S}}{(2N+1)^{1+S}}\\binom{S}{S_1}\\] となる．よって (a). 事後確率は \\[\\frac{(N+1)^{2+S}}{(2N+1)^{1+S}}\\binom{S}{S_1}\\left(1+\\frac{(N+1)^{2+S}}{(2N+1)^{1+S}}\\binom{S}{S_1}\\right)^{-1}\\]. (b). ベイズ因子は \\[\\left(\\frac{(N+1)^{2+S}}{(2N+1)^{1+S}}\\binom{S}{S_1}\\right)^{-1}.\\] "],["tab-2.html", "Chapter 2 乱数", " Chapter 2 乱数 Exercise 2.1 R言語の組み込み関数runifを用いて，長さ\\(1000\\)の疑似一様乱数\\(x_1, x_2,\\ldots, x_{1000}\\)を生成せよ．また，それらを\\(1\\times 1\\)の二次元正方形に配置せよ，すなわち， \\(xy\\)平面上に，\\(\\{(x_1, x_2), (x_3, x_4), \\ldots, (x_{999}, x_{1000})\\}\\)を描画せよ． u &lt;- matrix(runif(1000),nrow=2) data.fr &lt;- data.frame(x=u[1,], y=u[2,]) ggplot(data.fr,aes(x=x,y=y))+geom_point()+theme_bw() なお，三次元への配置は次のようにしてできる． set.seed(1234) data.fr &lt;- data.frame(x = runif(1e3), y = runif(1e3), z = runif(1e3)) plot_ly(data.fr, x=~x, y=~y, z=~z) %&gt;% add_markers(marker=list(size=3)) さらに，RAND法の配置は次のようにすれば良い．三次元に配置すると問題が起きることも見える． u &lt;- numeric(3*1e3) y &lt;- 1234 n &lt;- 2 ^ 31 - 1 a &lt;- 65539 b &lt;- 0 for(i in 1:length(u)){ y &lt;- (a * y + b) %% n u[i] &lt;- y/n } u_mat &lt;- matrix(u, nrow=3) data.fr &lt;- data.frame(x=u_mat[1,], y= u_mat[2,], z= u_mat[3,]) plot_ly(data.fr, x=~x, y=~y, z=~z) %&gt;% add_markers(marker=list(size=3)) Exercise 2.2 \\(a=13, b=0, n=67\\)とし，初期値\\(y_0=1234\\)として線形合同法を用いて，長さ\\(1000\\)の疑似一様乱数\\(x_1, x_2,\\ldots, x_{1000}\\)を生成せよ．また，上の問題と同じように，それらを\\(1\\times 1\\)の二次元正方形に配置せよ，また，描画された点のうち，異なる点の数を数え上げよ． u &lt;- numeric(1e3) y &lt;- 1234 n &lt;- 67 a &lt;- 13 b &lt;- 0 for(i in 1:length(u)){ y &lt;- (a * y + b) %% n u[i] &lt;- y/n } u_mat &lt;- matrix(u, nrow=2) data.fr &lt;- data.frame(x=u_mat[1,], y=u_mat[2,]) ggplot(data.fr,aes(x=x,y=y))+geom_point()+theme_bw() dim(unique(data.fr))[1] #No. of unique points ## [1] 33 Exercise 2.3 ロジスティック分布 (Logistic distribution)は \\[\\begin{equation}\\nonumber%\\label{eq:logistic_function} F(x)=\\frac{e^x}{1+e^x}\\ (-\\infty&lt;x&lt;\\infty) \\end{equation}\\] なる累積分布関数を持つ．R言語の組み込み関数runifを使ってロジスティック乱数を生成せよ．また，R言語の組み込み関数rlogisと計算時間の比較をおこなえ． Solution. 逆変換法を使おう．累積分布関数の逆関数は \\[ F^{-1}(u)=\\log \\frac{u}{1-u} \\] だから，\\(U\\sim\\mathcal{U}[0,1]\\)なら\\(F^{-1}(U)\\)がロジスティック分布に従う． f &lt;- function(u){ return(log( u / (1 - u) ) )} N &lt;- 1e5 microbenchmark(A &lt;- f(runif(N)), times = 100) ## Unit: milliseconds ## expr min lq mean median uq max neval ## A &lt;- f(runif(N)) 2.030293 2.038626 2.275785 2.080542 2.401063 4.866459 100 microbenchmark(B &lt;- rlogis(N), times = 100) ## Unit: milliseconds ## expr min lq mean median uq max neval ## B &lt;- rlogis(N) 2.501584 2.511959 2.61694 2.541605 2.608646 5.695917 100 組み込み関数が\\(10\\%\\)ほど速いようだ． Exercise 2.4 \\(a, b\\)を正の実数とする．パレート分布 (Pareto distribution) の確率密度関数が \\[ p(x;a,b)=\\frac{ab^a}{x^{a+1}}\\ (x\\ge b) \\] で与えられるとき，R言語の組み込み関数runifを用いてパレート乱数を生成する方法を記述せよ． Solution. まず累積分布関数を計算しよう．定義どおり積分すると，任意の\\(x\\ge b\\)に対し \\[\\begin{align*} F(x)&amp;=\\int_b^xp(y;a,b)\\mathrm{d}y\\\\ &amp;=\\int_b^x\\frac{ab^a}{y^{a+1}}\\mathrm{d}y\\\\ &amp;=b^a\\left[-y^{-a}\\right]_b^x =1-(x/b)^{-a}. \\end{align*}\\] したがって，累積分布関数の逆関数は \\[ F^{-1}(u)=b/(1-u)^{1/a} \\] となり，\\(F^{-1}(U)=b/(1-U)^{1/a}\\)で生成できる．ただし，\\(U\\)と\\(1-U\\)の従う確率分布は同じだから，\\(b/U^{1/a}\\)で生成できる． a &lt;- 2.0 b &lt;- 1.0 N &lt;- 1e5 f &lt;- function(x) a*b^a/(x^(a+1)) data.fr &lt;- data.frame(x = b/runif(N)^(1/a)) ggplot(data.fr, aes(x=x))+geom_histogram(aes(y=..density..), binwidth = 0.1, alpha=0.3)+theme_bw()+xlim(1,4)+stat_function(fun=f) ## Warning: Removed 6160 rows containing non-finite values (stat_bin). ## Warning: Removed 2 rows containing missing values (geom_bar). Exercise 2.5 \\(\\sigma\\)は正の実数とする．レイリー分布 (Rayleigh distribution) の確率密度関数は \\[ p(x;\\sigma)=\\frac{x}{\\sigma^2}\\exp(-x^2/2\\sigma^2)\\ (x\\ge 0) \\] で与えられる．R言語の組み込み関数runifを用いてレイリー乱数を生成する方法を記述せよ． sigma &lt;- 2.0 N &lt;- 1e5 f &lt;- function(x) x/sigma^2 * exp(-x^2/(2*sigma^2)) data.fr &lt;- data.frame(x = sigma*sqrt(-2*log(runif(N)))) ggplot(data.fr, aes(x=x))+geom_histogram(aes(y=..density..),alpha=0.3,binwidth=0.1)+theme_bw()+stat_function(fun=f) Exercise 2.6 \\(a, b\\)は正の実数とする．ワイブル分布 (Weibull distribution) の確率密度関数が \\[ p(x;a,b)=\\frac{a}{b^a}x^{a-1}e^{-(x/b)^a}\\ (x\\ge 0) \\] で与えられる．R言語の組み込み関数runifを用いてワイブル乱数を生成する方法を記述せよ． a &lt;- 1.5 b &lt;- 2.0 N &lt;- 1e5 f &lt;- function(x) a/b^a*x^(a-1)*exp(-(x/b)^a) data.fr &lt;- data.frame(x = b*(-log(runif(N)))^(1/a)) ggplot(data.fr, aes(x=x))+geom_histogram(aes(y=..density..),alpha=0.3,binwidth=0.1)+theme_bw()+stat_function(fun=f) Exercise 2.7 \\(N\\)は正の整数，\\(\\theta\\)は実数で\\(0&lt;\\theta&lt;1\\)とする．二項分布\\(\\mathcal{B}(N,\\theta)\\)の確率関数は \\[ p(n; N,\\theta)=\\binom{N}{n}\\theta^n(1-\\theta)^{N-n}\\ (n=0,\\ldots, N) \\] で与えられる．R言語の組み込み関数runifを用いて二項分布 に従う乱数を生成する方法を記述せよ． N &lt;- 10 theta &lt;- 0.6 F_seq &lt;- cumsum(dbinom(0:N,N,theta)) f &lt;- Vectorize(function(x) sum(x &gt; F_seq)) data.fr &lt;- data.frame(x=f(runif(1e4))) ggplot(data.fr, aes(x))+geom_bar(aes(y=..prop..), alpha=0.5)+theme_bw()+stat_function(geom=&quot;point&quot;, fun=dbinom, args=list(size=N, prob=theta), n=11) Exercise 2.8 \\(r\\)は正の整数，\\(\\theta\\)は\\(0&lt;\\theta&lt;1\\)を満たす実数とする．は確率関数 \\[\\begin{equation}\\label{eq:neg_binom} p(n;r,\\theta)=\\binom{n+r-1}{n}(1-\\theta)^r\\theta^n\\ (n=0,1,2,\\ldots) \\end{equation}\\] を持つ．次の手続きで負の二項乱数が生成できることを示せ．必要があれば例1.9を参考にせよ． r &lt;- 10 y &lt;- rgamma(1, shape = r, rate = (1-theta)/theta) x &lt;- rpois(1, y) Solution. 上のRコードによれば，\\(Y=y\\)の条件のもと，確率変数\\(X\\)の従う分布は ポアソン分布\\(\\mathcal{P}(y)\\)であり，確率関数は \\[ p(x\\mid y)=\\frac{y^x}{x!}\\exp(-y) \\] である．さらに\\(y\\)は\\(\\mathcal{G}(r, (1-\\theta)/\\theta)\\)に従う． \\(X\\)の周辺分布の確率密度関数は \\[\\begin{align*} \\int_0^\\infty p(x\\mid y)p(y)\\mathrm{d}y &amp;= \\int_0^\\infty \\frac{y^x}{x!}\\exp(-y)\\frac{1}{\\Gamma(r)}\\left(\\frac{1-\\theta}{\\theta}\\right)^ry^{r-1}\\exp\\left(-\\frac{1-\\theta}{\\theta}y\\right)\\mathrm{d}y\\\\ &amp;= \\frac{1}{x!\\Gamma(r)}\\left(\\frac{1-\\theta}{\\theta}\\right)^r~\\int_0^\\infty y^{x+r-1}\\exp\\left(-\\frac{1}{\\theta}y\\right)\\mathrm{d}y\\\\ &amp;= \\frac{\\Gamma(x+r)}{x!\\Gamma(r)}\\left(\\frac{1-\\theta}{\\theta}\\right)^r~\\theta^{x+r} \\end{align*}\\] となる．ただし，教科書の式(1.7)を使った． あとは\\(\\Gamma(r)=(r-1)!, \\Gamma(x+r)=(x+r-1)!\\)であることから，式を見比べて結論が得られる． Exercise 2.9 \\(\\alpha\\)は正の実数とする．歪正規分布 (Skew normal distribution) は確率密度関数 \\[ p(x)=2\\phi(x)\\Phi(\\alpha x)\\ (-\\infty&lt;x&lt;\\infty) \\] を持つ．ただし\\(\\phi(x)，\\Phi(x)\\)は標準正規分布の確率密度関数および累積分布関数である．次の手続きで歪正規乱数が生成できることを示せ． rho &lt;- alpha ^ 2 / ( 1 + alpha ^2 ) w &lt;- rnorm(2) y &lt;- sqrt( rho ) * w[1] + sqrt(1 - rho) * w[2] x &lt;- (y &gt;= 0) * w[1] - (y &lt; 0) * w[1] Solution. まず上の手続きを確率変数で書き直そう． \\(W_1, W_2\\)が独立で標準正規分布に従うとき， \\[\\begin{align*} Y&amp;=\\sqrt{\\rho}~W_1+\\sqrt{1-\\rho}~W_2\\\\ X&amp;= \\left\\{ \\begin{split} W_1&amp;\\quad Y\\ge 0\\\\ -W_1&amp;\\quad Y&lt;0 \\end{split} \\right. \\end{align*}\\] 定める．ただし\\(\\rho=\\alpha^2/(1+\\alpha^2)\\)である． 上の問題と同じように\\(X\\)の従う確率分布の周辺密度関数\\(p(x)\\)を計算したいが， \\(Y, W_1\\)を与えたもとでの\\(X\\)の分布が確定的なので同じようにはいかない．だから\\(X\\)の周辺分布の累積分布関数\\(F(x)\\)を求めて，それを微分することで周辺密度関数を導出しよう． 定義から \\[ F(x)=\\mathbb{P}(X\\le x)=\\mathbb{P}(W_1\\le x, Y\\ge 0)+ \\mathbb{P}(-W_1\\le x, Y&lt; 0) \\] である．じつは，右辺の２つの確率は同じだ．なぜなら\\(W_1\\)と\\(W_2\\)の符号を取り替えれば，ひとつめの事象とふたつめの事象が同じになるからだ．ここでは等号が入っているかどうかは小さいことなので気にしなくていい．だから． \\[ F(x)=2\\mathbb{P}(W_1\\le x, Y\\ge 0) \\] となる． いっぽう，\\(W_1=z\\)をあたえたもとで， \\[\\begin{align*} \\mathbb{P}(Y\\ge 0\\mid W_1=z)&amp;=\\mathbb{P}(\\sqrt{\\rho}~z+\\sqrt{1-\\rho}W_2\\ge 0)\\\\ &amp;=\\mathbb{P}\\left(W_2\\ge -\\sqrt{\\frac{\\rho}{1-\\rho}}z\\right)\\\\ &amp;=\\mathbb{P}\\left(W_2\\ge -\\alpha~z\\right)\\\\ &amp;=1-\\Phi(-\\alpha z)=\\Phi(\\alpha z) \\end{align*}\\] となる．ここで，\\(\\Phi(x)\\)は標準正規分布の累積分布関数で， \\(1-\\Phi(-x)=\\Phi(x)\\)を満たす．あとは\\(W_1=z\\)を\\(z\\le x\\)の範囲で積分すれば \\[ F(x)=2\\int_{-\\infty}^x\\Phi(\\alpha z)\\phi(z)\\mathrm{d}x \\] となる．両辺を\\(x\\)で微分すれば， \\[ p(x)=2\\Phi(\\alpha x)\\phi(x). \\] ただしここで \\[ \\left(\\int_{-\\infty}^xf(z)\\mathrm{d}z\\right)&#39;=f(x) \\] という事実を使った． Exercise 2.10 \\(\\lambda\\)は正の実数とする．ラプラス分布 (Laplace distribution) は確率密度関数 \\[ p(x;\\lambda)=\\frac{\\lambda}{2}\\exp(-\\lambda |x|)\\ (-\\infty&lt;x&lt;\\infty) \\] を持つ確率分布である．両側指数分布を棄却法の提案分布として標準正規乱数を生成することを考える．このとき\\(\\lambda\\)の関数として，式(2.6)の\\(R\\)を求め，\\(R\\)を最小にする\\(\\lambda\\)を求めよ． なお，式(2.6)とは \\[\\begin{align*} R=\\sup_{x\\in E}\\frac{p(x)}{q(x)} \\end{align*}\\] のことであり，本問の場合は\\(p(x)\\)が標準正規分布の確率密度関数 \\(\\phi(x)=\\exp(-x^2/2)/\\sqrt{2\\pi}\\)で，\\(q(x)\\)が\\(p(x;\\lambda)\\)である． Solution. 上の\\(R\\)を\\(\\lambda\\)の関数として， \\[\\begin{align*} R(\\lambda)&amp;=\\sup_{x\\in\\mathbb{R}}\\frac{\\phi(x)}{p(x;\\lambda)} \\end{align*}\\] とする．定義から \\[\\begin{align*} R(\\lambda)&amp;=\\sup_{x\\in\\mathbb{R}}\\frac{\\exp(-x^2/2)/\\sqrt{2\\pi}}{\\lambda 2^{-1}\\exp(-\\lambda|x|)}\\\\ &amp;= \\sqrt{2/\\pi}\\sup_{x\\ge 0}\\lambda^{-1}\\exp(-x^2/2+\\lambda x) \\end{align*}\\] となる．ただし，最後の等号では，\\(\\sup\\)の中身が\\(x=0\\)について対称であることを使った．指数関数の中身を平方完成すると \\[\\begin{align*} R(\\lambda)&amp;= \\sqrt{2/\\pi}\\sup_{x\\ge 0}\\lambda^{-1}\\exp(\\lambda^2/2)~\\exp(-(x-\\lambda)^2/2)\\\\ &amp;=\\sqrt{2/\\pi}~\\lambda^{-1}\\exp(\\lambda^2/2) \\end{align*}\\] である．あとは \\[ f(\\lambda)=\\log\\left(\\sqrt{2/\\pi}~\\lambda^{-1}\\exp(\\lambda^2/2)\\right) =\\log\\left(\\sqrt{2/\\pi}\\right)-\\log\\lambda+\\lambda^2/2 \\] となる．\\(f&#39;(\\lambda)=-1/\\lambda+\\lambda\\)であり， \\(\\lambda=1\\)で\\(f\\)は最小になることがわかる．よって\\(R(\\lambda)\\)を最小にする\\(\\lambda\\)は\\(\\lambda=1\\). maxr &lt;- sqrt(2/pi)*1*exp(1/2) # R(lambda) = sup r(lambda) r &lt;- function(x) dnorm(x)/(dexp(abs(x))/2) # r(lambda) n &lt;- 1e4 vec &lt;- numeric(1e2) for(i in 1:n){ x &lt;- rexp(1) * (2*rbinom(1,1,0.5)-1) # Initial trial while(runif(1) &gt; r(x)/maxr ){ # Rejection sampling x &lt;- rexp(1) * (2*rbinom(1,1,0.5)-1) } vec[i] &lt;- x } # Plot histgram vs density t &lt;- seq(-4,4,length=100) fit &lt;- dnorm(t) fig &lt;- plot_ly(x = vec, type=&quot;histogram&quot;, histnorm = &quot;probability density&quot;, name=&quot;rejection method&quot;) fig %&gt;% add_lines(x = t, y = fit, mode = &#39;lines&#39;, name=&quot;pdf&quot;) Exercise 2.11 (20211127追加）観測\\(x\\)は\\(x\\mid \\theta\\sim\\mathcal{N}(\\theta, 1)\\)によって生成され，パラメータ\\(\\theta\\)には事前分布\\(\\mathcal{C}(0,1)\\)が定められている． \\(\\mathcal{C}(0, 1)\\)を提案分布として，事後分布に従う乱数を棄却法を用いて生成せよ． Solution. 教科書p64の通りに進めれば良い．事後分布は \\[ p(\\theta\\mid x)= \\frac{p(x\\mid\\theta)p(\\theta)}{p(x)} \\] となる．提案分布として事前分布を使うと，事後分布と提案分布の確率密度関数の割合\\(r(\\theta)\\)は \\[ r(\\theta)=\\frac{p(x\\mid\\theta)}{p(x)} \\] となる．\\(r(\\theta)\\)は\\(\\theta=\\widehat{\\theta}=x\\)のとき最大になり，\\(p(x\\mid\\theta)=\\exp(-(x-\\theta)^2/2)/\\sqrt{2\\pi}\\)より，そのときの値は \\[ R=\\frac{p(x\\mid\\widehat{\\theta})}{p(x)}=\\frac{1}{\\sqrt{2\\pi}~p(x)} \\] となる．したがって， \\[ \\frac{r(\\theta)}{R}=\\frac{p(x\\mid\\theta)}{p(x\\mid\\widehat{\\theta})}=\\exp\\left(-\\frac{(x-\\theta)^2}{2}\\right) \\] となる．よって棄却法は次の手続きで実行できる． \\(\\theta\\sim\\mathcal{C}(0,1), U\\sim\\mathcal{U}[0,1]\\)とする． \\(U\\le \\exp\\left(-\\frac{(x-\\theta)^2}{2}\\right)\\)のとき\\(\\theta\\)を出力する． "],["tab-3.html", "Chapter 3 積分法", " Chapter 3 積分法 Exercise 3.1 \\(N\\)は正の整数，\\([0,1]\\)上で定義された関数\\(f(x)\\)の補間多項式としてバーンスタイン多項式 (Bernstein polynomial) \\[ g(x)=\\mathbb{E}\\left[f\\left(\\frac{X}{N}\\right)\\right]=\\sum_{n=0}^Nf\\left(\\frac{n}{N}\\right)x^n(1-x)^{N-n} \\] を考える．ただし\\(X\\sim \\mathcal{B}(N,x)\\). このとき， \\[ J=\\int_0^1 g(x)\\mathrm{d}x \\] を求めよ． なお，大数の法則から，\\(f(x)\\)が連続なら，\\(g(x)\\)は\\(f(x)\\)に一様収束することが知られている． Solution. ベータ関数の定義より \\[ \\int_0^1 x^n(1-x)^{N-n}\\mathrm{d}x=B(n+1, N-n+1) \\] となる．また， \\[\\begin{align*} B(n+1, N-n+1)&amp;=\\frac{\\Gamma(n+1)\\Gamma(N-n+1)}{\\Gamma(N+2)}\\\\ &amp;=\\frac{n!(N-n)!}{(N+1)!} \\end{align*}\\] であるから， \\[\\begin{align*} \\binom{N}{n}B(n+1, N-n+1)=\\frac{1}{N+1}. \\end{align*}\\] よって \\[ \\int_0^1g(x)\\mathrm{d}x=\\frac{1}{N+1}\\sum_{n=1}^Ng\\left(\\frac{n}{N}\\right). \\] Exercise 3.2 \\(N=3\\)とし，\\(a=x_0&lt;x_1&lt;x_2=b\\)を等間隔，すなわち\\(x_1=(a+b)/2\\)とした場合のルジャンドル多項式\\(l_1, l_2, l_3\\)にたいし \\[ w_i=\\int_a^b l_i(x)\\mathrm{d}x\\ (i=1,2,3) \\] を求めよ． Solution. 定義から， \\[\\begin{align*} l_1(x)&amp;=\\frac{\\left(x-\\frac{a+b}{2}\\right)(x-b)}{\\left(a-\\frac{a+b}{2}\\right)(a-b)} =2\\frac{\\left(x-\\frac{a+b}{2}\\right)(x-b)}{(a-b)^2}, \\\\ l_2(x)&amp;=\\frac{(x-a)(x-b)}{\\left(\\frac{a+b}{2}-a\\right)\\left(\\frac{a+b}{2}-b\\right)} =-4\\frac{(x-a)(x-b)}{(a-b)^2}, \\\\ l_3(x) &amp;=2\\frac{\\left(x-\\frac{a+b}{2}\\right)(x-a)}{(a-b)^2} \\end{align*}\\] となる．したがって \\[\\begin{align*} w_1=\\int_a^bl_1(x)\\mathrm{d}x&amp;=\\int_a^b\\frac{\\left\\{(x-a)+(x-b)\\right\\}(x-b)}{(a-b)^2}\\\\ &amp;=\\left(-\\frac{1}{6}+\\frac{1}{2}\\right)(b-a)\\\\ &amp;=\\frac{1}{3}(b-a) \\end{align*}\\] となり，同様の計算で\\(w_3=w_1=(b-a)/3\\)となる．また， \\[\\begin{align*} w_2=\\int_a^bl_2(x)\\mathrm{d}x&amp;=\\frac{2}{3}(b-a). \\end{align*}\\] Exercise 3.3 ニュートン・コーツの公式において，\\(N=3\\)とし，\\(a=x_0&lt;x_1&lt;x_2=b\\)を等間隔，すなわち\\(x_1=(a+b)/2\\)とした場合をという．区間\\([a,b]\\)上の関数\\(f(x)\\)に対するシンプソン公式が \\[ J=\\frac{b-a}{6}\\left(f(a)+4f\\left(\\frac{a+b}{2}\\right)+f(b)\\right) \\] で与えられることを示せ． Solution. 前問の結果と定義からただちにに従う． Exercise 3.4 複合台形公式を用いて(3.7)を計算せよ． N &lt;- 100 # No. of points -1 for trapezoidal rule f &lt;- function(x) (cos(50*x) + sin(20*x))^2 seq &lt;- (0:N) / N (sum(f(seq[-1]))+sum(f(seq[-(N+1)]))) / (2 * (N-1)) ## [1] 0.975289 Exercise 3.5 例3.6の\\(I\\)の値を，R言語の組み込み関数rcauchyを用いて，基本的モンテカルロ積分法で小数点以下第二位まで正確に求めよ． Solution. \\(X_1,\\ldots, X_M\\)をコーシー乱数としたとき， \\[ I_M=\\frac{1}{M}\\sum_{m=1}^M1_{\\{2\\le X_m\\}} \\] により近似できる．また，その推定量の分散は \\[ \\frac{1}{M-1}\\sum_{m=1}^M(f(x_m)-I_M)^2 \\] で近似できる．実際に計算すると # 95% Confidence interval CI &lt;- function(m){ x &lt;- (rcauchy(m)&gt;2) paste(&quot;estimate&quot;, mean(x), &quot;lower_bound&quot;, mean(x)-1.96*sd(x)/sqrt(m), &quot;upper_bound&quot;, mean(x)+1.96*sd(x)/sqrt(m)) } CI(1e6) ## [1] &quot;estimate 0.147178 lower_bound 0.146483604441074 upper_bound 0.147872395558926&quot; Exercise 3.6 任意の実数\\(x\\)にたいし，正接関数の逆関数\\(\\arctan x\\)を，R言語の組み込み関数rcauchyを用いた基本的モンテカルロ積分法で近似する方法を構成せよ．また，その値をR言語の組み込み関数atanと比較せよ． Solution. 例3.6の計算から， \\[\\begin{align*} \\int_x^\\infty\\frac{1}{\\pi(1+y^2)}\\mathrm{d}y &amp;= \\int_{\\arctan x}^{\\pi/2}\\frac{1}{\\pi}\\mathrm{d}\\theta=\\frac{1}{2}-\\frac{\\arctan x}{\\pi} \\end{align*}\\] である．よって \\[ \\arctan x= \\pi\\left\\{\\frac{1}{2}-\\int_x^\\infty\\frac{1}{\\pi(1+y^2)}\\mathrm{d}y\\right\\}. \\] 上の式に含まれる積分計算をモンテカルロ法で近似して図示しよう． N &lt;- 1e2 # No. of samples k &lt;- 1e2 # Points for plot x &lt;- rcauchy(N) # Random number f &lt;- ecdf(x) points &lt;- seq(-2, 2, length = k) data.fr &lt;- data.frame(x=points, y=pi*(f(points)-0.5)) ggplot(data.fr, aes(x=x,y=y,color=&quot;MC&quot;))+geom_step(aes(color=&quot;direct&quot;))+stat_function(fun=atan)+theme_bw()+scale_color_brewer(palette = &quot;Set1&quot;) Exercise 3.7 例3.6の計算でわかるように，\\(I\\)は \\[ f(x)=\\left\\{\\begin{array}{ll} 1&amp; \\mathrm{if}\\ x\\ge \\arctan 2\\\\ 0&amp; \\mathrm{if}\\ x&lt;\\arctan 2 \\end{array}\\right. \\] および\\(p(x)\\)を区間\\([-\\pi/2, \\pi/2]\\)の一様分布\\(\\mathcal{U}[-\\pi/2,\\pi/2]\\)の確率密度関数として \\[ I=\\int_{-\\pi/2}^{\\pi/2} f(x)p(x)\\mathrm{d}x \\] とも表現できる．この表現を利用してR言語の組み込み関数runifを用いた基本的モンテカルロ積分法を構成せよ．また，その誤差をrcauchyを用いた方法と理論的におよび数値的に比較せよ．正接関数の逆関数\\(\\arctan x\\)の導出はR言語の組み込み関数atanを用いよ． Solution. 誤差評価のため，分散を比較しよう．例3.6のように\\(f(x)\\)をとる．期待値の真の値は\\(I=0.5-(\\arctan 2)/\\pi\\)である．このとき，例3.6のrcauchyを用いた基本的モンテカルロ積分法での分散をまず計算する．確率変数\\(X\\)が\\(\\mathcal{C}(0,1)\\)に従うとして，定理3.4における\\(\\sigma^2\\)は \\[\\sigma^2=\\operatorname{Var}[f(X)]=\\mathbb{E}[f(X)^2]-\\mathbb{E}[f(X)]^2=I-I^2\\] となる．一方，runifを用いた方法では，練習問題3.7のように\\(f(x)\\)を取ると，確率変数\\(X\\)が\\(\\mathcal{U}[-\\pi/2,\\pi/2]\\)に従うとして やはりおなじように \\[\\sigma^2=\\operatorname{Var}[f(X)]=\\mathbb{E}[f(X)^2]-\\mathbb{E}[f(X)]^2=I-I^2\\] となる．じつは２つの方法はまったくおなじ手法である．例2.2より\\(U\\sim\\mathcal{U}[-\\pi/2,\\pi/2]\\)に従うなら \\(\\tan U\\)はコーシー分布に従うからだ． M &lt;- 1e2 N &lt;- 1e2 true &lt;- 0.5 - atan( 2 )/pi ## runif method set.seed(1234) v &lt;- numeric(N) for(i in 1:N){ u &lt;- runif(M, -pi/2, pi/2) v[i] &lt;- mean((u &gt; atan(2))) } mean((v-true)^2) ## [1] 0.0009072253 ## rcauchy method set.seed(1234) v &lt;- numeric(N) for(i in 1:N){ u &lt;- runif(M, -pi/2, pi/2) x &lt;- tan(u) v[i] &lt;- mean((x &gt; 2)) } mean((v-true)^2) ## [1] 0.0009072253 Exercise 3.8 長さ\\(N\\)の観測\\(x_1,\\ldots, x_N\\mid \\theta\\)は独立でコーシー分布\\(\\mathcal{C}(\\theta,1)\\)に従い，\\(\\theta\\)には\\(\\mathcal{C}(0,1)\\)が事前分布として仮定されているとする．このとき事後平均を，R言語の組み込み関数rcauchyによる自己正規化モンテカルロ法で計算せよ． M &lt;- 1e4 N &lt;- 10 theta &lt;- 2.0 x &lt;- theta + rcauchy(N) p1 &lt;- Vectorize(function(theta) prod(dcauchy(x-theta))*theta) p2 &lt;- Vectorize(function(theta) prod(dcauchy(x-theta))) theta &lt;- rcauchy(M) vec &lt;- p1(theta)/p2(theta) data.fr &lt;- data.frame(x=1:M, y = (cumsum(vec)/1:M)) ggplot(data.fr, aes(x=x, y=y))+geom_line()+theme_bw() Exercise 3.9 積分 \\[ I=\\int_{-\\infty}^\\infty \\exp\\left(-\\sqrt{|x|}\\right)(\\sin x)^2\\mathrm{d}x \\] を重点サンプリング法で計算する．コーシー分布\\(\\mathcal{C}(0,1)\\)を提案分布とする方法と，標準正規分布\\(\\mathcal{N}(0,1)\\)を提案分布とする方法を数値的に比較し，結果を考察せよ． Solution. 以下で数値実験を見れば，正規分布の方法が不安定なのがわかるだろう．なぜだろか．じつは，例3.10と同じように，正規分布を使った方法では分散が発散してしまう．計算はやや煩雑であるが，\\(M=1\\)としたときの推定量の分散は, \\(f(x)=\\exp\\left(-\\sqrt{|x|}\\right)(\\sin x)^2\\), \\(q(x)=\\exp(-x^2/2)/\\sqrt{2\\pi}\\)として， \\[\\int_{-\\infty}^\\infty\\left(\\frac{f(x)}{q(x)}\\right)^2q(x)\\mathrm{d}x-I^2=\\sqrt{2\\pi}\\int_{-\\infty}^\\infty\\exp(x^2/2-x)\\sin(x)^4\\mathrm{d}x-I^2\\] となる．正弦関数は\\(0\\)になることもあるのでわかりにくいが，右辺の積分は\\(+\\infty\\)となる．積分の不等式評価は本書の範囲を超えるから，数値的に大きさを確認できれば良い． しっかりとしめすには，まず \\[ J_k:=\\int_{k\\pi}^{(k+1)\\pi}\\sin(x)^4\\mathrm{d}x \\] が正であり，\\(k\\)に依存しないことに注意する．そして， \\(x\\ge 2\\)のとき\\(x^2/2-x\\ge 0\\)だから， \\[\\begin{align*} \\int_{-\\infty}^\\infty\\exp(x^2/2-x)\\sin(x)^4\\mathrm{d}x&amp;= 2\\int_0^\\infty\\exp(x^2/2-x)\\sin(x)^4\\mathrm{d}x\\\\ &amp;\\ge 2\\int_\\pi^\\infty\\sin(x)^4\\mathrm{d}x \\end{align*}\\] を示す．すると最後の式が\\(2(J_1+J_2+\\cdots)\\)と表されることと先程の注意から，この積分が\\(+\\infty\\)であることがわかる． set.seed(1234) M &lt;- 1e5 f &lt;- Vectorize(function(x) exp(-sqrt(abs(x))) * sin(x)^2) x &lt;- rcauchy(M) y1 &lt;- f(x)/dcauchy(x) # Cauchy distribution x &lt;- rnorm(M) y2 &lt;- f(x)/dnorm(x) # Normal distribution data.fr &lt;- data.frame(x=rep(1:M,2), y = c(cumsum(y1)/1:M,cumsum(y2)/1:M ), method = rep(c(&quot;Cauchy&quot;, &quot;Normal&quot;), each=M)) ggplot(data.fr, aes(x=x, y=y, color=method))+geom_line()+theme_bw() +scale_color_brewer(palette = &quot;Set1&quot;) "],["tab-4.html", "Chapter 4 マルコフ連鎖", " Chapter 4 マルコフ連鎖 Exercise 4.1 \\(m\\)は非負の整数，\\(N\\)は正の整数とする．また\\(x\\)は\\(0\\le x\\le N\\)なる整数とする．ライト・フィッシャーモデルに対し\\(\\mathbb{E}[X_{m+1}\\mid X_m=x]\\)および\\(\\operatorname{Var}(X_{m+1}\\mid X_m=x)\\)を求めよ． Solution. \\(X_{m+1}\\)は\\(X_m=x\\)で条件つけたもと\\(\\mathcal{B}(N, x/N)\\)に従う．よって二項分布の性質より \\[\\begin{align*} \\mathbb{E}[X_{m+1}\\mid X_m=x]&amp;=N\\frac{x}{N}=x\\\\ \\operatorname{Var}[X_{m+1}\\mid X_m=x]&amp;=N\\frac{x}{N}\\left(1-\\frac{x}{N}\\right)=N^{-1}x(N-x) \\end{align*}\\] Exercise 4.2 \\(N\\)は正の整数，\\(x\\)は\\(0\\le x\\le N\\)なる整数とする．ライト・フィッシャーモデルにたいし， \\[ \\mathbb{E}[X_m(N-X_m)\\mid X_m=x]=(1-N^{-1})x(N-x) \\] を示せ． Solution. 条件つき期待値，分散の計算から \\[\\begin{align*} \\mathbb{E}[X_m(N-X_m)\\mid X_m=x]&amp;=N\\mathbb{E}[X_{m+1}\\mid X_m=x]-\\mathbb{E}[X_{m+1}^2\\mid X_m=x]\\\\ &amp;=N\\mathbb{E}[X_{m+1}\\mid X_m=x]-(\\operatorname{V}[X_{m+1}\\mid X_m=x]+\\mathbb{E}[X_{m+1}\\mid X_m=x]^2)\\\\ &amp;=Nx-(N^{-1}x(N-x)+x^2)=(1-N^{-1})x(N-x) \\end{align*}\\] を得る． Exercise 4.3 \\(m\\)は非負の整数，\\(N\\)は正の整数とする．ライト・フィッシャーモデルに対し\\(\\operatorname{Cov}(X_{m+1}, X_m)=\\operatorname{Var}(X_m)\\)を示せ． Solution. \\[\\begin{align*} \\mathbb{E}[X_{m+1}\\mid X_m=x]=x \\end{align*}\\] だから，任意の\\(m=0,1,\\ldots\\)にたいし，\\(\\mathbb{E}[X_{m}]=\\mathbb{E}[X_{0}]\\)かつ， \\[\\begin{align*} \\mathbb{E}[X_{m+1}X_m] &amp;=\\sum_{x=0}^N\\mathbb{E}[X_{m+1}\\mid X_m=x]~x~\\mathbb{P}(X_m=x)\\\\ &amp;=\\sum_{x=0}^N~x^2~\\mathbb{P}(X_m=x)\\\\ &amp;=\\mathbb{E}[X_m^2]. \\end{align*}\\] よって \\[\\begin{align*} \\operatorname{Cov}(X_{m+1},X_m)&amp;=\\mathbb{E}[(X_{m+1}-x)(X_m-x)]\\\\ &amp;=\\mathbb{E}[X_{m+1}X_m]-x^2\\\\ &amp;=\\mathbb{E}[X_m^2]-x^2\\\\ &amp;=\\operatorname{Var}[X_m]. \\end{align*}\\] Exercise 4.4 ライト・フィッシャーモデルに対し \\[ \\lim_{m\\rightarrow\\infty}\\operatorname{Var}(X_m)=N\\mathbb{E}[X_0]-\\mathbb{E}[X_0]^2 \\] であることを示せ． Solution. 定理4.1の証明の中で \\[ \\mathbb{E}[X_m(N-X_m)]=(1-N^{-1})^m\\mathbb{E}[X_0(N-X_0)] \\] がえられている．したがって\\(m\\rightarrow\\infty\\)として \\[ \\lim_{m\\rightarrow\\infty}\\mathbb{E}[X_m(N-X_m)]=0 \\] である．よって\\(\\mathbb{E}[X_m]=\\mathbb{E}[X_0]\\)より \\[ \\lim_{m\\rightarrow\\infty}\\mathbb{E}[X_m^2]=N\\mathbb{E}[X_0]. \\] ゆえに \\[\\begin{align*} \\lim_{m\\rightarrow\\infty}\\operatorname{Var}[X_m]&amp;= \\lim_{m\\rightarrow\\infty}(\\mathbb[X_m^2]-\\mathbb{E}[X_m]^2)\\\\ &amp;=N\\mathbb{E}[X_0]-\\mathbb{E}[X_0]^2. \\end{align*}\\] Exercise 4.5 \\(m\\)は非負の整数，\\(x\\)は実数とする． 自己回帰過程に対し， \\(\\mathbb{E}[X_{m+1}\\mid X_m=x]\\)および\\(\\operatorname{Var}(X_{m+1}\\mid X_m=x)\\)を求めよ． Solution. 式(4.1)より \\[\\begin{align*} \\mathbb{E}[X_{m+1}\\mid X_m=x]&amp;=\\mathbb{E}[\\mu+\\alpha(X_m-\\mu)+W_m\\mid X_m=x]\\\\ &amp;=\\mu+\\alpha(x-\\mu). \\end{align*}\\] また \\[\\begin{align*} \\operatorname{Var}[X_{m+1}\\mid X_m=x]&amp;=\\operatorname{Var}[\\mu+\\alpha(X_m-\\mu)+W_m\\mid X_m=x]\\\\ &amp;=\\operatorname{Var}[W_m\\mid X_m=x]=\\sigma^2. \\end{align*}\\] Exercise 4.6 各\\(m=1,2,\\ldots\\)について\\(\\mu_m,\\mu\\)は実数，\\(\\sigma^2_m, \\sigma^2\\)は正の実数とする．また，\\(\\mu_m\\rightarrow\\mu, \\sigma^2_m\\rightarrow\\sigma^2\\)とする．このとき， \\[\\begin{align*} \\mathcal{N}(\\mu_m,\\sigma^2_m)~\\longrightarrow~\\mathcal{N}(\\mu,\\sigma^2) \\end{align*}\\] となることを示せ（特性関数が収束することを見ればいい）． Solution. 確率分布の収束は，その特性関数の収束と同値である．正規分布\\(\\mathcal{N}(\\mu,\\sigma^2)\\)の特性関数は \\[ \\varphi(t)=\\exp\\left(\\mu t\\mathbf{i}-\\frac{\\sigma^2}{2}t^2\\right) \\] であるから， \\[ \\exp\\left(\\mu_m t\\mathbf{i}-\\frac{\\sigma_m^2}{2}t^2\\right) \\longrightarrow \\exp\\left(\\mu t\\mathbf{i}-\\frac{\\sigma^2}{2}t^2\\right) \\] より結論を得る． Exercise 4.7 \\(x, \\lambda\\)は正の実数，\\(m\\)は正の整数とする．確率分布\\(G=\\mathcal{N}(0,1)\\)としたときの\\(\\mathbb{R}_+\\)上のランダムウォークにたいし， \\[ \\mathbb{E}[\\exp(-\\lambda X_{m+1})\\mid X_m=x] \\] を求めよ． Solution. （20211127解答修正） \\(X_m=x\\)のもと，\\(X_{m+1}=\\max\\{0, x+W_{m+1}\\}\\)で\\(W_{m+1}\\)は標準正規分布に従う． \\[\\begin{align*} \\exp(-\\lambda X_{m+1})= \\begin{cases} 1&amp;\\mathrm{if}\\ W_{m+1}\\le -x\\\\ \\exp(-\\lambda(x+W_{m+1})&amp;\\mathrm{if}\\ W_{m+1}&gt;-x \\end{cases} \\end{align*}\\] である．したがって， \\[\\begin{align*} \\mathbb{E}[\\exp(-\\lambda X_{m+1})\\mid X_m=x]&amp;= \\mathbb{P}(W_{m+1}\\le -x)+\\int_{-x}^\\infty\\exp(-\\lambda(x+w))\\frac{1}{\\sqrt{2\\pi}}\\exp(-w^2/2)\\mathrm{d}w\\\\ &amp;=\\Phi(-x)+\\exp(-\\lambda~x+\\lambda^2/2)\\int_{-x}^\\infty\\frac{1}{\\sqrt{2\\pi}}\\exp(-(w+\\lambda)^2/2)\\mathrm{d}w \\end{align*}\\] となる．ただし，\\(\\Phi(x)=1-\\Phi(-x)\\)は標準正規分布の累積分布関数である．よって \\(u=w+\\lambda\\)と変数変換すれば \\[\\begin{align*} \\mathbb{E}[\\exp(-\\lambda X_{m+1})\\mid X_m=x] &amp;=\\Phi(-x)+\\exp(-\\lambda~x+\\lambda^2/2)\\int_{-x+\\lambda}^\\infty\\frac{1}{\\sqrt{2\\pi}}\\exp(-w^2/2)\\mathrm{d}w\\\\ &amp;=\\Phi(-x)+\\exp(-\\lambda~x+\\lambda^2/2)(1-\\Phi(-x+\\lambda)) \\end{align*}\\] を得る． Exercise 4.8 例4.6のマルコフ連鎖にたいし，\\(\\gamma&lt;1\\)のとき \\[ \\lim_{m\\rightarrow\\infty}\\mathbb{E}[X_m\\mid X_0=x] \\] および \\[ \\lim_{m\\rightarrow\\infty}\\operatorname{Var}[X_m\\mid X_0=x] \\] を求めよ． Solution. （20211127解答修正） \\(\\gamma=\\alpha^2+\\beta^2\\sigma^2&lt;1\\)より，とくに\\(|\\alpha|&lt;1\\)である． マルコフカーネルの定義から \\[ \\mathbb{E}[X_m]=\\alpha\\mathbb{E}[X_{m-1}]=\\cdots=\\alpha^m\\mathbb{E}[X_0] \\] より，\\(m\\rightarrow\\infty\\)とすると \\[\\begin{align*} \\lim_{m\\rightarrow\\infty}\\mathbb{E}[X_m]=0 \\end{align*}\\] を得る．一方，例4.6の途中の式から， \\[\\begin{align*} \\lim_{m\\rightarrow\\infty}\\mathbb{E}[X_m^2+\\delta X_m\\mid X_0=x] &amp;= \\sum_{i=0}^\\infty\\gamma^i\\sigma^2=\\frac{\\sigma^2}{1-\\gamma} \\end{align*}\\] をえる．したがって， \\[\\begin{align*} \\lim_{m\\rightarrow\\infty}\\mathbb{E}[X_m^2\\mid X_0=x]&amp;= \\lim_{m\\rightarrow\\infty}\\mathbb{E}[X_m^2+\\delta X_m\\mid X_0=x]-\\delta \\lim_{m\\rightarrow\\infty}\\mathbb{E}[ X_m\\mid X_0=x]\\\\ &amp;=\\frac{\\sigma^2}{1-\\gamma}. \\end{align*}\\] よって，分散は \\[\\begin{align*} \\lim_{m\\rightarrow\\infty}\\mathbb{V}[X_m\\mid X_0=x]&amp;= \\lim_{m\\rightarrow\\infty}\\mathbb{E}[X_m^2\\mid X_0=x]- \\lim_{m\\rightarrow\\infty}\\mathbb{E}[ X_m\\mid X_0=x]^2\\\\ &amp;=\\frac{\\sigma^2}{1-\\gamma}. \\end{align*}\\] Exercise 4.9 例4.7の自己回帰過程において，\\(\\alpha=-1\\)のとき不変確率分布が存在しないことを示せ． Solution. 例4.7の式(4.10)より，もし不変確率分布\\(\\Pi\\)が存在するなら，その特性関数を\\(\\psi(u)\\)と書くと \\[ \\psi(u)=\\psi(-u)\\exp(-u^2\\sigma^2/2)=\\psi(u)\\exp(-u^2\\sigma^2) \\] を得る．したがって\\(u\\)が\\(0\\)でないなら， \\(\\exp(-u^2\\sigma^2)&lt;1\\)より，必然的に\\(\\psi(u)=0\\)でなければならない．すると特性関数の連続性から \\(\\psi(0)=0\\)の必要がある．いっぽう，これは特性関数のもう一つの性質である\\(\\psi(0)=1\\)に矛盾している．したがって不変分布は存在しない． Exercise 4.10 実数\\(\\mu\\)に対し， \\[\\begin{align*} \\|\\mathcal{N}(0,1)-\\mathcal{N}(\\mu,1)\\|_{\\mathrm{TV}} \\end{align*}\\] を求めよ． Solution. （20211127解答修正） 定義から \\[\\begin{align*} \\|\\mathcal{N}(0,1)-\\mathcal{N}(\\mu,1)\\|_{\\mathrm{TV}}=\\frac{1}{2}\\int_{-\\infty}^\\infty|\\phi(x)-\\phi(x-\\mu)|\\mathrm{d}x \\end{align*}\\] である．ここで， \\[\\begin{align*} \\begin{cases} \\phi(x)\\ge \\phi(x-\\mu)&amp;\\mathrm{if}\\ \\|x\\|\\le \\|x-\\mu\\|\\\\ \\phi(x) &lt;\\phi(x-\\mu)&amp;\\mathrm{if}\\ \\|x\\|&gt; \\|x-\\mu\\| \\end{cases} \\end{align*}\\] だから， \\[\\begin{align*} \\|\\mathcal{N}(0,1)-\\mathcal{N}(\\mu,1)\\|_{\\mathrm{TV}}=\\frac{1}{2}\\int_{\\|x\\|\\le \\|x-\\mu\\|}(\\phi(x)-\\phi(x-\\mu))\\mathrm{d}x + \\frac{1}{2}\\int_{\\|x\\|&gt; \\|x-\\mu\\|}(\\phi(x-\\mu)-\\phi(x))\\mathrm{d}x \\end{align*}\\] となる．この２つの積分を計算して足し合わせればよいが，じつはこの２つの積分は同じ値になる．なぜなら，これらが図4.7の青い領域，緑の領域それぞれを表すからだ．だから，２つの積分のうち，片方を求めて二倍すればよい．したがって求める値は\\(\\mu\\ge 0\\)のとき \\[\\begin{align*} \\int_{\\|x\\|\\le \\|x-\\mu\\|}(\\phi(x)-\\phi(x-\\mu))\\mathrm{d}x&amp;= \\int_{-\\infty}^{\\mu/2}(\\phi(x)-\\phi(x-\\mu))\\mathrm{d}x\\\\ &amp;=\\Phi(\\mu/2)-\\Phi(-\\mu/2). \\end{align*}\\] 同様の計算を\\(\\mu&lt;0\\)の場合に施すことにより，結局一般の\\(\\mu\\)について，求める値は \\[ |\\Phi(\\mu/2)-\\Phi(-\\mu/2)|=1-2\\Phi(-|\\mu|/2). \\] Exercise 4.11 実数\\(\\sigma^2&gt;1\\)に対し， \\[\\begin{align*} \\|\\mathcal{N}(0,1)-\\mathcal{N}(0,\\sigma^2)\\|_{\\mathrm{TV}} \\end{align*}\\] を求めよ． Solution. 前の問題と同じように，定義から \\[\\begin{align*} \\|\\mathcal{N}(0,1)-\\mathcal{N}(\\mu,1)\\|_{\\mathrm{TV}}=\\frac{1}{2}\\int_{-\\infty}^\\infty|\\phi(x)-\\sigma^{-1}\\phi(x/\\sigma)|\\mathrm{d}x \\end{align*}\\] である．ここで \\[\\begin{align*} \\phi(x)\\ge \\sigma^{-1}\\phi(x/\\sigma)\\Longleftrightarrow 2\\left(1-\\sigma^{-2}\\right)^{-1}\\log \\sigma\\ge x^2 \\end{align*}\\] となる．実数\\(\\eta=\\left(2\\left(1-\\sigma^{-2}\\right)^{-1}\\right)^{1/2}\\)とすると，前の問題と同じ議論により， \\[\\begin{align*} \\|\\mathcal{N}(0,1)-\\mathcal{N}(\\mu,1)\\|_{\\mathrm{TV}}&amp;=\\frac{1}{2}\\int_{\\|x\\|\\le \\eta}\\left(\\phi(x)-\\sigma^{-1}\\phi(x/\\sigma)\\right)\\mathrm{d}x + \\frac{1}{2}\\int_{\\|x\\|&gt; \\eta}\\left(\\sigma^{-1}\\phi(x/\\sigma)-\\phi(x)\\right)\\mathrm{d}x\\\\ &amp;= \\int_{\\|x\\|\\le \\eta}\\left(\\phi(x)-\\sigma^{-1}\\phi(x/\\sigma)\\right)\\mathrm{d}x\\\\ &amp;= \\Phi(\\eta)-\\Phi(-\\eta)-\\Phi(\\eta/\\sigma)+\\Phi(\\eta/\\sigma). \\end{align*}\\] Exercise 4.12 \\(P, Q\\)が確率関数\\(p(x), q(x)\\)を持つとき， \\[ \\sum_{x\\in E}\\min\\{p(x), q(x)\\}&gt;0 \\] なら，\\(P, Q\\)は互いに特異ではない．とくに，\\(p(x)&gt;0, q(x)&gt;0\\)なる\\(x\\)があれば\\(P, Q\\)は互いに特異ではないことを示せ． Solution. 仮定は，ある\\(x^*\\)があって，\\(\\min\\{p(x^*),q(x^*)\\}&gt;0\\)となるということである．いっぽう，もし\\(P, Q\\)が特異なら，ある集合\\(A\\)があって， \\(P(A)=Q(A^c)=0\\)となる．もし\\(x^*\\)が\\(A\\)に含まれるなら， \\[ P(A)\\ge p(x^*)&gt;0 \\] となり矛盾するし，もし\\(A^c\\)に含まれるなら \\[ Q(A)\\ge q(x^*)&gt;0 \\] となりやはり矛盾する．だから，\\(P, Q\\)は互いに特異ではない． "],["tab-5.html", "Chapter 5 ギブスサンプリング", " Chapter 5 ギブスサンプリング Exercise 5.1 実数\\(\\theta, q_0, q_1\\)は\\(0&lt;\\theta, q_0, q_1&lt;1\\)を満たすとする． \\(P_0=\\mathcal{B}(1,q_0), P_1=\\mathcal{B}(1,q_1)\\)とするとき，混合分布\\(P_\\theta\\)をベルヌーイ分布を用いて表わせ． Solution. \\(P_\\theta=\\mathcal{B}(1,(1-\\theta) q_0+\\theta q_1)\\). Remark. すなわち，ベルヌーイの定理分布の混合も再びベルヌーイ分布になる．混合分布を考えても分布の世界が広がらないのである． Exercise 5.2 定理5.2を特性関数を用いて示せ．なお，\\(\\sigma_0, \\sigma_1\\)は存在するものとする． Solution. \\(\\varphi_0, \\varphi_1\\)をそれぞれ\\(P_0, P_1\\)の特性関数とする．このとき，\\(P_\\theta\\)の特性関数は \\[ \\varphi_\\theta=(1-\\theta)\\varphi_0+\\theta\\varphi_1 \\] となる．平均は\\(\\mu_\\theta=\\varphi_\\theta&#39;(0)\\)であり，分散は\\(\\sigma_\\theta^2=-\\varphi_\\theta&#39;&#39;(0)-(\\varphi_\\theta&#39;(0))^2\\)で表される．この事実より， \\[\\begin{align*} \\mu_\\theta &amp;=\\varphi_\\theta&#39;(0)\\\\ &amp;=(1-\\theta)\\varphi_0&#39;(0)+\\theta\\varphi_1&#39;(0)\\\\ &amp;=(1-\\theta)\\mu_0+\\theta\\mu_1. \\end{align*}\\] また，\\(i=0,1\\)について \\[ \\varphi_i&#39;&#39;(0)=-\\sigma_i^2-(\\varphi_i&#39;(0))^2 \\] より \\[\\begin{align*} \\sigma_\\theta^2 &amp;=-\\varphi_\\theta&#39;&#39;(0)-(\\varphi_\\theta&#39;(0))^2\\\\ &amp;=-\\left((1-\\theta)\\varphi_0&#39;&#39;(0)+\\theta\\varphi_1&#39;&#39;(0)\\right)-\\left((1-\\theta)\\varphi_0&#39;(0)+\\theta\\varphi_1&#39;(0)\\right)^2\\\\ &amp;=-\\left((1-\\theta)(-\\sigma_0^2-\\mu_0^2)+\\theta(-\\sigma_1^2-\\mu_1^2)\\right)-((1-\\theta)\\mu_0+\\theta\\mu_1)^2 \\end{align*}\\] となり，これを整理することで示せる． Exercise 5.3 条件つき確率(5.1), (5.2)を導出せよ． Solution. \\(x_n, I_n\\)の\\(\\theta\\)のもとでの結合分布の密度関数は \\[ p(x_n, I_n=i\\mid \\theta)= \\begin{cases} \\theta p_1(x_n)&amp;\\mathrm{if}\\ i=1\\\\ (1-\\theta) p_0(x_n)&amp;\\mathrm{if}\\ i=0 \\end{cases} \\] である．したがって， 情報\\(I^N\\)があるばあいの尤度は \\[\\begin{align*} p(x^N, I^N\\mid \\theta)=\\prod_{n=1}^N(\\theta p_1(x_n))^{I_n}((1-\\theta) p_0(x_n))^{1-I_n} \\end{align*}\\] となる．事後分布の導出に，\\(\\theta\\)を含まない項を除くと \\[\\begin{align*} p(x^N, I^N\\mid \\theta)\\propto \\prod_{n=1}^N \\theta^{I_n}(1-\\theta)^{1-I_n}=\\theta^{m_1}(1-\\theta_0)^{m_0} \\end{align*}\\] となる．よって，事前分布を\\(p(\\theta)=1\\)として入れると，事後密度関数は \\[\\begin{align*} p(\\theta\\mid x^N, I^N)\\propto p(x^N, I^N\\mid \\theta)p(\\theta)\\propto\\theta^{m_1}(1-\\theta_0)^{m_0} \\end{align*}\\] となり，これはベータ分布\\(\\mathcal{B}(m_0+1,m_1+1)\\)の確率密度関数の形である．したがって \\(I^N\\)の情報もあるときの事後分布はベータ分布\\(\\mathcal{B}e(m_0+1,m_1+1)\\)である. いっぽう，\\(I_n\\)の条件付き確率は先程の結合分布から \\[ \\mathbb{P}(I_n=1\\mid x_n,\\theta)=\\frac{p(x_n, I_n=1\\mid \\theta)}{p(x_n\\mid \\theta)}=\\frac{\\theta p_1(x_n)}{\\theta p_1(x_n)+(1-\\theta) p_0(x_n)} \\] を得る．またこの値を\\(1\\)から引くことにより\\(\\mathbb{P}(I_n=0\\mid x_n,\\theta)\\)もわかる． Exercise 5.4 \\(N, n\\)を正の整数，\\(\\theta\\)を実数で \\(1\\le n\\le N, 0&lt;\\theta&lt;1\\)とする．また\\(x_1,\\ldots, x_N\\)は実数，\\(I_1,\\ldots, I_N\\)は\\(0\\)もしくは\\(1\\)とする．このとき， 条件つき確率(5.1), (5.2)のもと， \\(\\mathbb{E}[~\\theta~\\mid x^N, I^N], \\mathbb{E}[~I_n~\\mid x^N,\\theta]\\)を求めよ． Solution. ベータ分布の性質から \\[ \\mathbb{E}[~\\theta~\\mid x^N, I^N]=\\frac{m_1+1}{(m_0+1)+(m_1+1)}=\\frac{m_1+1}{N+2}. \\] いっぽう， \\[ \\mathbb{E}[~I_n~\\mid x^N,\\theta]=\\mathbb{P}(I_n=1\\mid x_n,\\theta)=\\frac{\\theta p_1(x_n)}{\\theta p_1(x_n)+(1-\\theta) p_0(x_n)}. \\] Exercise 5.5 定理5.3を示せ． Solution. 略． Exercise 5.6 実数\\(\\mu\\)に対し，\\(X\\sim\\mathcal{N}^+(\\mu,1)\\)のとき，\\(\\mathbb{E}[X]\\)を求めよ． Solution. 仮定から \\[\\begin{align*} \\mathbb{E}[X]&amp;=\\frac{\\int_0^\\infty x\\phi(x-\\mu)\\mathrm{d}x}{\\Phi(\\mu)}\\\\ &amp;=\\mu+\\frac{\\int_0^\\infty (x-\\mu)\\phi(x-\\mu)\\mathrm{d}x}{\\Phi(\\mu)}\\\\ &amp;=\\mu+\\frac{\\left[\\phi(x-\\mu)\\right]_0^\\infty}{\\Phi(\\mu)}\\\\ &amp;=\\mu+\\frac{\\phi(-\\mu)}{\\Phi(\\mu)}=\\mu+\\frac{\\phi(\\mu)}{\\Phi(\\mu)} \\end{align*}\\] を得る． Exercise 5.7 例5.1で定義されるマルコフカーネルについて，\\(\\mathcal{N}(0,1)\\)が不変分布であることを示せ． Solution. \\(X\\sim \\mathcal{N}(0,1)\\)かつ\\(Y\\sim \\mathcal{N}(\\rho^2X,1-\\rho^4)\\)としたとき， \\(Y\\sim \\mathcal{N}(0,1)\\)であることを示せば良い．正規分布の再生性を使って示そう． \\(\\epsilon_1, \\epsilon_2\\)は独立な標準正規乱数とする．すると上で定義される\\(X,Y\\)の同時分布と \\[ X=\\epsilon_1, Y=\\rho^2\\epsilon_1+\\sqrt{1-\\rho^4}\\epsilon_2 \\] とおいたときの\\(X,Y\\)の同時分布はおなじになる．後者であれば再生性からただちに\\(Y\\sim \\mathcal{N}(0,1)\\)が得られる．よって\\(\\mathcal{N}(0,1)\\)は不変分布， Exercise 5.8 \\(x\\)は正の整数，\\(y\\)は正の実数とする．\\(X\\)は\\(Y=y\\)で条件づけたもと，\\(\\mathcal{P}(y)\\)に従う．また\\(Y\\)は\\(\\mathcal{E}(1)\\)に従う．このとき\\(Y\\)の\\(X=x\\)を固定したもとでの条件つき分布を導出し，ギブスサンプリングを構成せよ． Solution. \\(X, Y\\)の同時密度関数が \\[ p(x, y)=p(x\\mid y)p(y)=\\left\\{\\frac{y^x}{x!}\\exp(-y)\\right\\}\\exp(-y) \\] となる．したがって\\(Y\\)の\\(X=x\\)を固定したもとでの条件つき分布の確率密度関数は \\[ p(y\\mid x)\\propto p(x,y)\\propto y^x\\exp(-2y) \\] となる．これはガンマ分布\\(\\mathcal{G}(x+1,2)\\)の確率密度関数だから，条件付き分布は\\(\\mathcal{G}(x+1,-2)\\)である．実際にプログラムしてみよう． N &lt;- 1e3 # No. of iteration N1 &lt;- 1e3 # No. of iteration for trajectory plot lag &lt;- 25 # Lag of MCMCs vecx &lt;- numeric(N) # Output #1 vecy &lt;- numeric(N) # Output #2 x0 &lt;- rpois(1,1) # Initial value # Gibbs Sampler x &lt;- x0 for(i in 1:N){ y &lt;- rgamma(1,shape = x+1, rate = 2) x &lt;- rpois(1,y) vecx[i] &lt;- x vecy[i] &lt;- y } data.fr &lt;- melt(data.frame(x = c(vecx, vecy), m = rep(1:N,2), xy = factor(rep(c(&quot;x&quot;,&quot;y&quot;),each=N))), id =c(&quot;m&quot;,&quot;xy&quot;)) ggplot(data.fr, aes(x = m, y = value, color = xy)) + geom_path(show.legend = FALSE) + theme_bw() + facet_grid( . ~ xy)+ scale_color_brewer(palette = &quot;Set1&quot;) + ggtitle(&quot;Trajectory plot&quot;) Exercise 5.9 例5.2で定義されるギブスカーネル\\(P(x,\\cdot)\\)が負の超幾何分布 \\(\\mathcal{N}hg(2N+\\alpha+\\beta-1, N, x+\\alpha)\\) であることを示せ． Solution. \\(P(x,\\cdot)\\)の確率関数を\\(p(x,y)\\)と書くと \\[\\begin{align*} p(x,y)&amp;=\\int_0^1\\binom{N}{y}z^y(1-z)^{N-y}\\frac{z^{x+\\alpha-1}(1-z)^{N-x+\\beta-1}}{B(x+\\alpha, N-x+\\beta)}\\mathrm{d}z \\end{align*}\\] となる．ベータ関数の定義から，これは \\[\\begin{align*} p(x,y)&amp;=\\binom{N}{y}\\frac{B(x+y+\\alpha,2N-x-y+\\beta)}{B(x+\\alpha, N-x+\\beta)} \\end{align*}\\] となる．負の二項分布と見比べるために，さらに右辺を書き直すと， \\[\\begin{align*} p(x,y)&amp;=\\frac{N!}{y!(N-y)!}\\frac{(x+y+\\alpha-1)!(2N-x-y+\\beta-1)!}{(2N+\\alpha+\\beta-1)!}\\frac{(N+\\alpha+\\beta-1)!}{(x+\\alpha-1)!(N-x+\\beta-1)!}\\\\ &amp;=\\frac{N!}{y!(N-y)!}\\frac{(x+y+\\alpha-1)!(2N-x-y+\\beta-1)!}{(2N+\\alpha+\\beta-1)!}\\frac{(N+\\alpha+\\beta-1)!}{(x+\\alpha-1)!(N-x+\\beta-1)!}\\\\ &amp;=\\frac{\\binom{y+(x+\\alpha)-1}{y}\\binom{(2N+\\alpha+\\beta-1)-(x+\\alpha)-y}{N-y}}{\\binom{2N+\\alpha+\\beta-1}{N}} \\end{align*}\\] となり，\\(\\mathcal{N}hg(2N+\\alpha+\\beta-1, N, x+\\alpha)\\)の確率関数と一致する． Exercise 5.10 練習問題1.9 を参考に， 第5.1節の有限混合モデルのギブスサンプラーの拡張を考える． \\(K\\)は正の整数，\\(\\theta_1,\\ldots, \\theta_{K-1}\\)は正の実数で， \\[ \\sum_{k=1}^{K-1}\\theta_k&lt;1 \\] を満たすとする．また，\\(P_1,\\ldots, P_K\\)は確率分布で，確率密度関数\\(p_1,\\ldots, p_K\\)を持つ．確率分布\\(P_\\theta\\)は，確率密度関数 \\[ p_\\theta(x)=\\theta_1 p_1(x)+\\cdots+\\theta_Kp_K(x) \\] を持つとする．ただし，\\(\\theta_K=1-\\theta_1-\\cdots-\\theta_K\\)とする．ある正の整数\\(N\\)にたいし観測\\(x_1,\\ldots, x_N\\mid \\theta\\sim P_\\theta\\)となり，\\(\\theta\\sim\\mathcal{D}(\\alpha_1,\\ldots,\\alpha_K)\\)なる事前分布を入れる．このとき，ギブスサンプリングを構成せよ．なお，\\(\\alpha_1,\\ldots,\\alpha_K\\)は既知の正の実数とする． Solution. \\(K=2\\)の場合と同様に，\\(I_n=k, n=1,\\ldots\\)が，のとき，分布\\(P_k\\)からの乱数であることを明示する確率変数としよう．すると， \\(I^N=\\{I_1,\\ldots, I_N\\}\\)のもと，尤度は \\[ p(x^N, I^N\\mid \\theta)\\propto \\prod_{k=1}^K \\theta_k^{m_k} \\] となる．ただし，\\(m_k=\\sum_{n=1}^N1_{\\{I_n=k\\}}\\)とする．したがって事前分布として \\(\\mathcal{D}(\\alpha_1,\\ldots,\\alpha_K)\\)を入れると，事後密度関数は \\[ p(\\theta\\mid x^N, I^N)\\propto p(x^N, I^N\\mid \\theta)p(\\theta)\\propto \\prod_{k=1}^K \\theta_k^{m_k+\\alpha_k-1} \\] となり，これは\\(\\mathcal{D}(m_1+\\alpha_1,\\ldots,m_K+\\alpha_K)\\)の確率密度関数である．したがって \\(\\theta\\)の\\(I^N\\)も既知としたときの事後分布は\\(\\mathcal{D}(m_1+\\alpha_1,\\ldots,m_K+\\alpha_K)\\)である． 一方，\\(\\theta, x^N\\)のもとでの\\(I_n\\)の事後分布は多項分布 \\[ \\mathbb{P}(I_n=k\\mid x_n,\\theta)=\\frac{\\theta_k p_k(x_n)}{\\sum_{l=1}^K\\theta_l p_l(x_n)}\\ (k=1,\\ldots, K, n=1,\\ldots, N) \\] となる．したがってギブスサンプリングは \\(\\mathcal{D}(m_1+\\alpha_1,\\ldots,m_K+\\alpha_K)\\)と，うえの\\(I_n\\)の条件付き分布である多項分布を交互に繰り返すことで構成できる． "],["tab-6.html", "Chapter 6 メトロポリス・ヘイスティングス法", " Chapter 6 メトロポリス・ヘイスティングス法 Exercise 6.1 メトロポリス・ヘイスティングス法の採択関数\\(\\alpha(x,y)\\)の代わりに \\[ \\beta(x,y)=\\frac{\\pi(y)q(y,x)}{\\pi(x)q(x,y)+\\pi(y)q(y,x)} \\] を用いても定理6.1，定理6.2と同じ仮定のもと，同じ結論が従うことを示せ． Solution. メトロポリス・ヘイスティングスカーネルの\\(\\alpha(x,y)\\)を\\(\\beta(x,y)\\)でおきかえたマルコフカーネルを\\(P\\)と書く． まず定理6.1と同じ結論が従うことをみよう． 定理6.1の証明と同じように， \\[ \\int_{x\\in A, y\\in B}\\pi(x)q(x,y)\\beta(x,y)\\mathrm{d}x\\mathrm{d}y= \\int_{x\\in A, y\\in B}\\pi(y)q(y,x)\\beta(y,x)\\mathrm{d}x\\mathrm{d}y \\] を示せば良い．被積分関数に注目すると \\[ \\pi(x)q(x,y)\\beta(x,y)= \\frac{\\pi(y)q(y,x)\\pi(x)q(x,y)}{\\pi(x)q(x,y)+\\pi(y)q(y,x)} \\] となり\\(x, y\\)について対称な関数であることがわかる．だから，\\(x, y\\)を入れ替えても同じ値であり，この事実から示したい等式がわかる．したがって\\(P\\)は\\(\\Pi\\)-対称であり，定理4.8より\\(\\Pi\\)-不変でもある． つぎに定理6.2を示そう．これについても定理6.2の証明とまったく同じように\\(\\beta(x,y)&gt;0\\ (x,y\\in E)\\)であることから \\[ P(x, A)\\ge \\int_Aq(x,y)\\beta(x,y)\\mathrm{d}y=:k(x,y)&gt;0 \\] によって定理4.6から結論が従う．よって\\(P\\)はエルゴード的． Remark. 本問のように作られたマルコフカーネルを，しばしばバーカーカーネル(See Barker 1965)という．中心極限定理の漸近分散の観点から，バーカーカーネルは必ずメトロポリス・ヘイスティングスカーネルより良くない(See Tierney 1998)．しかし\\(\\beta(x,y)\\)は微分可能性など，関数としてよりなめらかなため，理論的に解析しやすい． Exercise 6.2 例6.1で紹介された二つの独立型メトロポリス・ヘイスティングス法のそれぞれに対し，式(6.1)で定義される\\(M\\)を計算せよ． Solution. 定義から \\[ M=C\\sqrt{2\\pi}\\sup_{x\\in\\mathbb{R}^d}\\frac{1}{1+\\theta^2}=C\\sqrt{2\\pi}. \\] Exercise 6.3 独立型メトロポリス・ヘイスティングス法を \\(\\Pi=\\mathcal{N}(0,1)\\), \\(Q=\\mathcal{C}(0,1)\\)としたものと， \\(\\Pi=\\mathcal{C}(0,1)\\), \\(Q=\\mathcal{N}(0,1)\\)としたものを実装し，結果を比較せよ． Solution. 実際に，ふたつのメトロポリス・ヘイスティングス法から，長さNのサンプルを生成し，経路図と採択率，事項相関図を生成してみよう． N &lt;- 1e5 # No. of iteration N1 &lt;- 1e3 # No. of iteration for trajectory plot lag &lt;- 25 # Lag of MCMCs pi1 &lt;- function(x) dnorm(x) # Target #1 q1 &lt;- function(x) dcauchy(x) # Proposal #1 w1 &lt;- function(x) pi1(x)/q1(x) # Ratio #1 w2 &lt;- function(x) 1/w1(x) # Ratio #2 vec1 &lt;- numeric(N) # Output #1 vec2 &lt;- numeric(N) # Output #2 x0 &lt;- numeric(1) # Initial value # MCMC #1 x &lt;- x0 for(i in 1:N){ y &lt;- rcauchy(1) if(runif(1) &lt; w1(y)/w1(x)){ x &lt;- y } vec1[i] &lt;- x } # MCMC #2 x &lt;- x0 for(i in 1:N){ y &lt;- rnorm(1) if(runif(1) &lt; w2(y)/w2(x)){ x &lt;- y } vec2[i] &lt;- x } data.fr &lt;- melt(data.frame(x = c(vec1, vec2), m = rep(1:N,2), method = factor(rep(c(1,2),each=N))), id =c(&quot;m&quot;,&quot;method&quot;)) ggplot(subset(data.fr, m &lt; N1), aes(x = m, y = value, color=method)) + geom_path() + theme_bw() + scale_color_brewer(palette = &quot;Set1&quot;) + ggtitle(&quot;Trajectory plot&quot;) paste(&quot;Acceptance probability #1&quot;, sum(diff(c(x,vec1))!=0)/N) ## [1] &quot;Acceptance probability #1 0.70507&quot; paste(&quot;Acceptance probability #2&quot;, sum(diff(c(x,vec2))!=0)/N) ## [1] &quot;Acceptance probability #2 0.77516&quot; acf1 &lt;- acf(vec1, lag = lag, plot = FALSE) acf2 &lt;- acf(vec2, lag = lag, plot = FALSE) M &lt;- length(acf1$lag) data.fr &lt;- data.frame(lag = rep(as.vector(acf1$lag),2), y = c(as.vector(acf1$acf), as.vector(acf2$acf)), z = factor(rep(c(1,2), each=M))) ggplot(data.fr , aes(x = lag, y = y, fill=z)) + geom_bar(stat = &quot;identity&quot;, position = position_dodge(), alpha=1.0) + theme_bw() + scale_fill_brewer(palette = &quot;Set1&quot;) + ggtitle(&quot;Acf plot&quot;) Remark. 前者は一様エルゴード的，後者はそうでないことが知られる．自己相関関数の推移をみるかぎり，理論的な結果と整合的である． Exercise 6.4 \\(\\mathbb{R}^d\\)上の確率測度\\(\\Pi\\)に対し，確率密度関数\\(\\pi(x)\\)があるとする．メトロポリス・ヘイスティングス法で 正の実数\\(\\rho,\\sigma\\)で\\(0&lt;\\rho&lt;1\\)なるものに対し， \\(Q(x,\\cdot)=\\mathcal{N}(\\rho^{1/2}x, (1-\\rho)\\sigma^2I_d)\\)とする． このとき採択関数が \\[ \\alpha(x,y)=\\min\\left\\{1,\\frac{\\pi(y)\\phi_\\sigma(x)}{\\pi(x)\\phi_\\sigma(y)}\\right\\} \\] で与えられることを示せ．ただし，\\(\\phi_\\sigma(x)\\)は\\(\\mathcal{N}(0,\\sigma^2 I_d)\\)の確率密度関数とする． Solution. メトロポリス・ヘイスティングス法の採択関数の定義とみくらべて \\[ \\frac{q(y,x)}{q(x,y)}=\\frac{\\phi_\\sigma(x)}{\\phi_\\sigma(y)} \\] であること，すなわち \\[ \\phi_\\sigma(x)q(x,y)=\\phi_\\sigma(y)q(y,x) \\] を示せば良い．ただし\\(q(x,y)\\)は\\(Q(x,\\cdot)=\\mathcal{N}(\\rho^{1/2}x, (1-\\rho)\\sigma^2I_d)\\)の確率密度関数 \\[ q(x,y)=\\frac{1}{\\sqrt{2\\pi\\sigma^2(1-\\rho)}}\\exp\\left(-\\frac{(y-\\rho^{1/2}x)^2}{2\\sigma^2(1-\\rho)}\\right) \\] である．いっぽう， \\[\\begin{align*} \\phi_\\sigma(x)q(x,y)=\\frac{1}{2\\pi\\sigma^2\\sqrt{(1-\\rho)}}\\exp\\left(-\\frac{x^2+y^2-2\\rho^{1/2}xy}{2\\sigma^2(1-\\rho)}\\right) \\end{align*}\\] となり，これは\\(x, y\\)を入れ替えても同じ値になる．よって\\(\\phi_\\sigma(x)q(x,y)=\\phi_\\sigma(y)q(y,x)\\)が示され，結論が従う． Remark. 上で紹介したマルコフカーネルは，しばしばpreconditioned Crank-Nikolsonカーネルとよばれ，高次元で，正規分布を事前分布として用いるときの解析に有効である(See Neal 1998, Cotter et al 2013)．関連するアルゴリズムとして，Elliptical slice samplingがある． Exercise 6.5 定理6.4を証明せよ． Solution. 定理6.2よりただちにしたがう．なぜなら\\(q(x,y)=\\gamma(y-x)&gt;0\\)となるからだ． Exercise 6.6 定理6.6を示せ． Solution. 略 Exercise 6.7 ハミルトン方程式の離散近似は様々な方法がある． オイラー法は \\[\\begin{align*} x_h&amp;=x_0+h\\left(\\frac{\\partial H}{\\partial w}\\right)(x_0, w_0), \\\\ w_h&amp;=w_0-h\\left(\\frac{\\partial H}{\\partial x}\\right)(x_0, w_0) \\end{align*}\\] で定義される．ただし，正の実数\\(h\\)はステップ幅．ハミルトニアンが\\(H(x,w)=(x^2+w^2)/2\\)であるとき， \\[ H(x_h,w_h)-H(x_0, w_0) \\] を\\(H(x_0,w_0)\\)を用いて表せ． Solution. （20211127解答修正） \\[\\begin{align*} H(x_h, w_h)-&amp;= \\frac{(x_0+hw_0)^2+(w_0-hx_0)^2}{2}\\\\ &amp;=\\frac{(x_0^2+w_0^2)+h^2(x_0^2+w_0^2)}{2}\\\\ &amp;=(1+h^2)H(x_0, w_0) \\end{align*}\\] とあらわせる．したがって， \\[ H(x_h,w_h)-H(x_0, w_0)=h^2H(x_0,w_0). \\] Remark. 上の結果から，オイラー法を繰り返すと，\\(k=1,2,\\ldots\\)にたいし \\[ H(x_{kh},w_{kh})=(1+h^2)^kH(x_0, w_0) \\] となり，指数的にハミルトニアンが大きくなってしまう．ハミルトニアン方程式の解はハミルトニアンを変えないから，これはオイラー法による近似の誤差がとても大きいことを示している． Exercise 6.8 同様に修正されたオイラー法では， ハミルトニアンが\\(H(x,w)=(x^2+w^2)/2\\)であるとき， \\[ H(x_h,w_h)-H(x_0, w_0)=\\frac{h^2(-x_0^2+w_h^2)}{2} \\] を示せ． Solution. （20211127解答追加） 修正されたオイラー法では \\[\\begin{align*} \\begin{pmatrix} x_h\\\\w_h \\end{pmatrix} = \\begin{pmatrix} w_0-hx_0\\\\x_0+h(w_0-hx_0) \\end{pmatrix} \\end{align*}\\] なる遷移を行う．したがって \\[\\begin{align*} H(x_h, w_h)-H(x_0, w_0)&amp;=\\frac{(w_0-hx_0)^2+(x_0+hw_h)^2}{2}-\\frac{x_0^2+w_0^2}{2}\\\\ &amp;=\\frac{h^2x_0^2-2hx_0(w_0-w_h)+h^2w_h^2}{2}\\\\ &amp;=\\frac{-h^2x_0^2+h^2w_0}{2}. \\end{align*}\\] Exercise 6.9 同様に馬跳び法の場合，ハミルトニアンが\\(H(x,w)=(x^2+w^2)/2\\)であるとき， \\[ \\begin{pmatrix} x_h\\\\ w_h \\end{pmatrix} =A_h \\begin{pmatrix} x_0\\\\ w_0 \\end{pmatrix} \\] となる行列\\(A_h\\)を求めよ． Solution. （20211127解答修正） ３つのステップを行列で表すと，それぞれ \\[\\begin{align*} \\begin{pmatrix} 1&amp;h/2 \\\\ 0&amp;1 \\end{pmatrix}, \\begin{pmatrix} 1&amp;0 \\\\ -h&amp;1 \\end{pmatrix}, \\begin{pmatrix} 1&amp;h/2 \\\\ 0&amp;1 \\end{pmatrix} \\end{align*}\\] をひだりから縦ベクトル\\((x,w)\\)に作用させることに対応する．よって求める行列は \\[\\begin{align*} \\begin{pmatrix} 1&amp;h/2 \\\\ 0&amp;1 \\end{pmatrix}~ \\begin{pmatrix} 1&amp;0 \\\\ -h&amp;1 \\end{pmatrix}~ \\begin{pmatrix} 1&amp;h/2 \\\\ 0&amp;1 \\end{pmatrix} = \\begin{pmatrix} 1-h^2/2&amp;h-h^3/4 \\\\ -h&amp;1-h^2/2 \\end{pmatrix}. \\end{align*}\\] "],["tab-error.html", "Chapter 7 エラータム 7.1 エラータム第一刷 7.2 エラータム 第二刷 7.3 エラータム第三刷", " Chapter 7 エラータム 7.1 エラータム第一刷 残念ながら本書（第一刷）でいくつかの誤りが見つかっている． 修正点の指摘について松野舜介氏に感謝します． 7.1.1 p44 例2.1について (20210308) 図2.4のキャプション：「パラメータ\\(p=2\\)の…」とあるのは「パラメータ\\(\\lambda=2\\)の指数乱数のヒストグラム．黒い実線は指数分布の確率密度関数」の誤り． 7.1.2 p48 例2.3について (20200511) 実数の繰り上げ，繰り下げに関する議論に誤りがあった． \\(\\lceil x\\rceil\\)で\\(x\\)未満ではない最小の整数を表すとする．このとき\\(1-(1-p)^x&lt;U\\)を満たす最大の整数は， \\[ 1-U&lt;(1-p)^x\\leadsto\\log(1-U)&lt; x\\log(1-p)\\leadsto \\frac{\\log(1-U)}{\\log(1-p)}&gt;x \\] より， \\[ x=\\left\\lceil\\frac{\\log(1-U)}{\\log(1-p)}-1\\right\\rceil \\] とすべきだ．だから，乱数は \\[ \\left\\lceil\\frac{\\log U}{\\log(1-p)}-1\\right\\rceil \\] で生成できる．R言語では p &lt;- 0.2 n &lt;- 3 ceiling(log(runif(n))/log(1-p)-1) ## [1] 1 0 1 とすべきだった．ただし，教科書にある \\[ \\left [\\frac{\\log U}{\\log(1-p)}\\right] \\] と先程の値が異なる確率は\\(0\\)だから，乱数生成の意味では問題を生じ得ない． 7.1.3 p70 定理3.2について (20210308) \\(l_1(x)\\)と\\(l_2(x)\\)の定義が逆． 7.1.4 p93 練習問題3.2について (20210308) \\(\\int_a^b(x-a)^2\\mathrm{d}x=\\int_a^b(x-b)^2\\mathrm{d}x=(b-a)^3/3\\)とすべき． 7.1.5 p95 章のアブストラクト (20210308) 第６行目；「マルコ連鎖」とあるのは「マルコフ連鎖」の誤り． 7.1.6 p109 例4.6について (20210308) 恥ずかしながら一部計算ミスしていた．以下に差し替える． 実数\\(\\alpha,\\beta\\)および正の実数\\(\\sigma\\)を定める．\\(\\alpha^2+\\beta^2\\sigma^2\\neq \\alpha\\)とする．初期値\\(X_0=x_0\\)にたいし，\\(m=0,1,\\ldots\\)として \\[\\begin{equation}\\label{eq:nonlinear} X_{m+1}=\\alpha X_m+\\beta X_m W_{m+1}+W_{m+1} \\end{equation}\\] なる非線形なモデルを考えよう．ただし，\\(W_1, W_2,\\ldots\\)は独立で\\(\\mathcal{N}(0,\\sigma^2)\\)に従う．この場合，マルコフカーネルは \\[ P(x,\\cdot)=\\mathcal{N}(\\alpha x, (1+\\beta x)^2\\sigma^2) \\] で定まる．定義式は自己回帰過程と少し違うだけだが，振る舞いはかなり違う．たとえば\\(P(x,\\cdot)\\)は正規分布だが，\\(\\beta\\neq 0, x\\neq 0\\)なら\\(P^2(x,\\cdot)\\)はもはや正規分布ではない． 自己回帰過程のように，長期的な振る舞いを検討してみよう．まず，\\(X_0=x\\)であるとき \\[\\begin{equation}\\label{eq:nonlinear_moment} \\mathbb{E}[X_1\\mid X_0=x]=\\alpha x,\\ \\mathbb{E}[X_1^2\\mid X_0=x]=\\alpha^2x^2+(1+\\beta x)^2\\sigma^2 \\end{equation}\\] となる．したがって，うまく計算すれば， \\[\\begin{align*} \\mathbb{E}[X_1^2+\\delta X_1\\mid X_0=x]=\\sigma^2 +\\gamma(x^2+\\delta x) \\end{align*}\\] となるのがわかるはずだ．ただし，\\(\\gamma=\\alpha^2+\\beta^2\\sigma^2\\)および\\(\\delta=2\\beta\\sigma^2/(\\gamma-\\alpha)\\)である．この関係式を再帰的に使えば \\[ \\begin{align*} \\mathbb{E}[X_m^2+\\delta X_m\\mid X_0=x]&amp;=\\sigma^2+\\gamma~\\mathbb{E}[X_{m-1}^2+\\delta X_{m-1}\\mid X_0=x]\\\\ &amp;=\\cdots= \\sum_{i=0}^{m-1} \\gamma^i\\sigma^2+\\gamma^m~(x^2+\\delta x) \\end{align*} \\] である．だから，\\(X_m^2+\\delta X_m\\)の期待値が発散しないためには\\(\\gamma=\\alpha^2+\\beta^2\\sigma^2&lt;1\\)が必要十分だ．なお，\\(X_m^2+\\delta X_m\\)の期待値が発散しないこととと\\(X_m^2\\)の期待値が発散しないことは同じことである．自己回帰過程の場合は，\\(X_m^2\\)の期待値が発散しないための条件は\\(\\alpha^2&lt;1\\)だった．それよりよりやや厳しい条件が必要だ． 7.1.7 p139 定理5.7について (20210308) 「\\(x\\in, y\\in F\\)」とあるのは 「\\(x\\in E, y\\in F\\)」の誤り． また，「練習問題4.13から」とあるのは「練習問題4.12から」の誤り． 7.1.8 p140 5.3.3について (20210308) 「第5.1節であつかったギブスサンプリングは\\(E=\\mathbb{R}\\)」 とあるのは 「第5.1節であつかったギブスサンプリングは\\(E=(0,1)\\)」の誤り． 7.1.9 p144 例5.7 について (20210308) 第12行目で\\(p(x^N, y^N\\mid \\theta)\\)とあるのは \\(p(x^N\\mid y^N,\\theta)\\)の誤り． また，下から二行目，正規分布の分散の\\(\\tau^{-1}\\)を掛けるのを忘れている． 7.1.10 p146 練習問題 5.10について (20210308) 「練習問題1.8」とあるのは「練習問題1.9」の誤り． 7.1.11 p155 下から8行目 (20210308) \\[ P(x,A)=(1-M^{-1})Q(x,A)+M^{-1}\\Pi(A) \\] が正しい． 7.1.12 p156 6.3.1について (20210308) 上から十行目 \\[ \\min\\left\\{1,\\frac{\\pi(y)}{\\pi(x)}\\right\\} \\] とすべき． 7.1.13 p159 6.3.2について (20210308) 上から3行目 \\[ \\frac{1}{M}\\sum_{m=1}^N\\|X_m-X_{m-1}\\|^2 \\] とすべき． また，Step 2で\\(U\\le \\alpha(X_{n-1}, Y_n)\\)とあるのは \\(U_n\\le \\alpha(X_{n-1}, Y_n)\\)の誤り． 7.1.14 p160 引き続き6.3.2について (20210308) 下から6,7行目 \\[ \\sum_{i=1}^n X_{0i}w_{1i},\\quad \\sum_{i=1}^n X_{0i}\\frac{w_{1i}}{\\|w_1\\|} \\] はそれぞれ \\[ \\sum_{i=1}^d X_{0i}w_{1i},\\quad \\sum_{i=1}^d X_{0i}\\frac{w_{1i}}{\\|w_1\\|} \\] の誤り． 7.1.15 p161 引き続き6.3.2について (20210308) 下から11行目 \\[ \\sigma^2=\\frac{l}{d} \\] ではなく \\[ \\sigma^2=\\frac{l^2}{d}. \\] 7.1.16 p164 リスト6.3について (20210308) 恥ずかしながら，プログラムにバグが有り，ニュートン・ラフソン法がよく見えているが，実際はもっと悪い．コードを直し，以下のように初期値を十分真値に近く取り直す． set.seed(1234) N &lt;- 100 theta &lt;- c(0.3,0.4) x &lt;- matrix(c(rep(1,N),rnorm(N)), nrow = 2, byrow = TRUE) p &lt;- exp(as.vector(theta%*% x))/(1+exp(as.vector(theta%*% x))) u &lt;- runif(N) y &lt;- as.numeric(u &lt;= p) f &lt;- function(theta){ sum(as.vector(theta %*% x) * y)-sum(log(1 + exp(as.vector(theta %*% x)))) } df &lt;- function(theta){ rowSums(t(t(x)*y)) - rowSums(t(t(x) * exp(as.vector(theta %*% x))/(1 + exp(as.vector(theta %*% x))))) } ddf &lt;- function(theta){ prob &lt;- exp(as.vector(theta%*% x))/(1+exp(as.vector(theta%*% x))) -x %*% diag(prob*(1-prob))%*% t(x) } data.grid &lt;- expand.grid(s.1 = seq(-5, 8, length.out=200), s.2 = seq(-5, 11, length.out=200)) pv &lt;- numeric(0) for(i in 1:(dim(data.grid)[1])){ pv &lt;- append(pv,f(as.numeric(data.grid[i,]))) } M &lt;- 20 theta0 &lt;- c(-1.5,1) theta &lt;- theta0 theta_mat &lt;- matrix(0, nrow=M, ncol=2) theta_mat[1,] &lt;- theta0 for(m in 2:M){ theta &lt;- theta - as.vector(solve(ddf(as.vector(theta))) %*% df(theta)) theta_mat[m,] &lt;- theta } htheta &lt;-theta data.fr &lt;- data.frame(x=theta_mat[,1],y=theta_mat[,2], z=1:M) q.samp &lt;- data.frame(cbind(data.grid, z = pv)) ggplot(q.samp, aes(x=s.1, y=s.2, z=z)) + geom_contour(color=black,alpha=0.5) + geom_path(data=data.fr,aes(x=x,y=y),lty=2,color=black,alpha=0.5) + geom_point(data=data.fr,aes(x=x,y=y),color=black,alpha=0.5,size=I(2.0))+xlim(-5, 8) + ylim(-5, 11) + xlab(TeX(&quot;$\\\\theta_1$&quot;)) + ylab(TeX(&quot;$\\\\theta_2$&quot;)) + theme_bw() 7.1.16.1 定義6.1 p169 について (20200511) 大事な定義だが間違っていた．たいへん大きなミスだ．文章はあっているが，その下の式が何から何まで間違っている．以下の式がただしい． \\[ H(x,w)=-\\log\\pi(x)-\\log\\phi_\\sigma(w)=-\\log\\pi(x)+\\|w\\|^2/2\\sigma^2+\\log(2\\pi\\sigma^2)/2. \\] 7.2 エラータム 第二刷 7.2.1 p21 例1.8の後の文章 (20200511) 事後確率の計算の直前「すなわち\\(\\theta&gt;0\\)」とあるのは「すなわち\\(\\theta&gt;0.5\\)」の誤り． 7.2.2 p49 例2.4の後の文章 (20211125) \\(F(n)\\)の式の分子の指数部は\\(n\\)でなく，\\(m\\)． 7.2.3 p57 定理2.7の後の証明の途中式 (20211125) \\(p(z)\\)の式の\\(p(y)\\)（青色)の部分は\\(2^{\\nu/2}\\)ではなく\\(2^{-\\nu/2}\\). 7.2.4 p66 練習問題2.8 コードが一部違っている． r &lt;- 10 y &lt;- rgamma(1, shape = r, rate = (1-theta)/theta) x &lt;- rpois(1, y) 7.2.5 p77 図3.7のキャプション (20211125) 青色の \\(f(x)\\)は\\(g(x)\\)の誤り． 7.2.6 p100 定理4.2のあとの解説 (20211125) 「さらに\\(|\\alpha|&gt;1\\)となると，\\(x\\neq \\mu\\)なら平均も発散する」とあるが，\\(x\\)ではなく\\(x_0\\)とすべき． 7.2.7 p 125, 126 定理5.2のあとの解説 (20211125) p125の(5.1)式およびp126の最初の式，さらにp126のギブスサンプリングStep2において\\(m_0\\)と\\(m_1\\)が反対になっている． 7.2.8 p 159 6.3.2 について (20211125) アルゴリズムStep 2において\\(X_{n+1}=X_n\\)は\\(X_n=X_{n-1}\\)の誤り． 7.2.9 p 176 練習問題6.9 について (20211127) \\(h^2(-x_0^2+w_h^2)\\)ではなく，\\(\\frac{h^2(-x_0^2+w_h^2)}{2}\\). 7.3 エラータム第三刷 7.3.1 p 31 リスト1.5について 13行目の sqrt の適用範囲が誤り．正しくは下記． data(&quot;ToothGrowth&quot;) x &lt;- ToothGrowth$len y &lt;- ToothGrowth$supp sd &lt;- sd(x) n1 &lt;- sum(y==&quot;OJ&quot;) x1 &lt;- x[y==&quot;OJ&quot;] mu1 &lt;- sum(x1) / (1 + n1) sd1 &lt;- sd * sqrt(1 + sum(y==&quot;OJ&quot;)) n2 &lt;- sum(y==&quot;VC&quot;) x2 &lt;- x[y==&quot;VC&quot;] mu2 &lt;- sum(x2) / (1 + n2) sd2 &lt;- sd * sqrt(1 + sum(y==&quot;VC&quot;)) q &lt;- 1/(1 + sqrt((1 + n1 + n2) / ((1 + n1) * (1 + n2))) * exp(-(sum(x^2)/(1+n1+n2) - sum(x1^2)/(1+n1)-sum(x2^2)/(1+n2))/(2*sd^2))) # Model Posterior probability of Model mu1=mu0 log10(q/(1-q)) # BayesFactor 7.3.2 p 35 リスト1.6について 説明変数の中心化が抜けている．対応する図も少し変わる． data(&quot;airquality&quot;) airquality x &lt;- airquality$Solar.R[is.na(airquality$Solar.R)+is.na(airquality$Ozone)==0] x &lt;- x - mean(x) #&lt;&lt; y &lt;- airquality$Ozone[is.na(airquality$Solar.R)+is.na(airquality$Ozone)==0] sd &lt;- 31.33 ggplot(airquality,aes(x = Ozone)) + stat_function(fun=dnorm, args = list(mean = sum(y)/(1+length(y)), sd = sd/sqrt(1+length(y))),color = &quot;blue&quot;)+theme_bw()+xlim(35,50) ggplot(airquality,aes(x = Ozone)) + stat_function(fun=dnorm, args = list(mean = sum(x*y)/(1+sum(x^2)), sd = sd/sqrt(1+sum(x^2))),color = &quot;blue&quot;)+theme_bw()+xlim(0,0.28) log10(exp(1/(2*sd^2)*sum(x*y)^2/(1+sum(x^2)))*1/sqrt(1+sum(x^2)))#Bayes Factor 7.3.3 p 41 線形合同法について 「擬似乱数法RANDUは \\(a=65539, b=0, n=2^{31}-1\\)とした線形合同法」とあるのは， 「擬似乱数法RANDUは \\(a=65539, b=0, \\color{red}{n=2^{31}}\\)とした線形合同法」の誤り． 7.3.4 p62 2.2.3 近似累積分布関数による乱数生成 rnormの出力が，runifの出力をqnorm関数で変換した逆関数法にように記述しているが，正確には異なる． 実際のrnormのコードによると #define BIG 134217728 /* 2^27 */ /* unif_rand() alone is not of high enough precision */ u1 = unif_rand(); u1 = (int)(BIG*u1) + unif_rand(); return qnorm5(u1/BIG, 0.0, 1.0, 1, 0); と書かれており，内部で二つの擬似一様乱数を生成して精度を担保している．二つの疑似乱数のうち最初の項が主要項なので，奇数番目に生成した乱数に逆変換法を用いた値はrnormにほとんど一致するが，細かいところでは異なる．以下のように，細かく見ると差を確認できる． m &lt;- 3 set.seed(1) print(rnorm( m ), digits = 10) ## [1] -0.6264538107 0.1836433242 -0.8356286124 set.seed(1) print(qnorm( c( runif(2*m)[ seq(from = 1, to = 2*m - 1, by = 2) ] ) ), digits = 10) ## [1] -0.6264538071 0.1836433242 -0.8356286213 7.3.5 p75 例2.7 ガンマ乱数の生成 ガンマ分布\\(\\mathcal{G}(\\nu, 1)\\)からの乱数の生成において，\\(\\nu &gt;0\\)の場合の乱数生成を\\(\\mathcal{G}([\\nu],1)\\)からの乱数による棄却法で生成できるように書いているが誤り． \\(\\nu&lt;1\\)の場合はこの方法は使えない． 7.3.6 p82 ５行目は「ラグランジュ多項式」ではなく「ルジャンドル多項式」 7.3.7 p84 定理3.6の証明 \\(Y_M/Z_M\\)とすべきところが\\(Z_M/Y_M\\)となっていた． \\[ I_M=I~\\frac{Y_M}{Z_M}\\approx I~\\left(1-(Z_M-1)+(Y_M-1)\\right) \\] とすべき．合わせて次の式も \\[ M^{1/2}(I_M-I)\\approx I~\\left( -M^{1/2}(Z_M-1)+M^{1/2}(Y_M-1)\\right)=M^{\\color{red}{-}1/2} I\\sum_{m=1}^M(-\\overline{g}(X_m)+\\overline{f}(X_m)) \\] とする．\\(M\\)の指数も誤り．次の式も以下のようになる． \\[\\begin{align*} \\sigma^2&amp;=I^2\\operatorname{Var}(-\\overline{g}(X_1)+\\overline{f}(X_1))\\\\ &amp;=I^2\\left\\{\\operatorname{Var}(\\overline{f}(X_1))-2 \\operatorname{Cov}(\\overline{f}(X_1),\\overline{g}(X_1)) +\\operatorname{Var}(\\overline{g}(X_1))\\right\\} \\end{align*}\\] 7.3.8 p93 3.1 \\(\\Gamma(n)=(n-1)!\\)とすべき． 7.3.9 p93 3.2 \\(a=x_1&lt;x_2&lt;x_3=b\\)および\\(x_2=(a+b)/2\\)とすべき． 7.3.10 p145 例5.7 \\(y_n\\sim\\chi^2_\\nu\\)の部分等で誤りがあった．第六行目は \\[ p(y_n\\mid x_n,\\theta)\\propto p(x_n,y_n\\mid\\theta)\\propto y_n^{1/2}\\exp\\left(-\\left\\{\\frac{(x_n-\\mu)^2}{2\\nu} \\tau\\right\\}y_n\\right) \\color{red}{y_n^{\\nu/2-1}\\exp\\left(-\\frac{y_n}{2}\\right)} \\] とし，次の式とStep 1のガンマ分布は \\[ \\mathcal{G}\\left(\\frac{1+\\nu}{2}, \\frac{(x_n-\\mu)^2}{2\\nu} \\tau+\\frac{1}{2}\\right) \\] とすべき．またStep 2は\\(\\tau\\)が抜けている． \\(\\mu\\sim \\mathcal{N}\\left(\\frac{\\left(\\nu^{-1} \\sum_{n=1}^Nx_n y_n\\right)}{1+\\nu^{-1}\\sum_{n=1}^N y_n},\\left(1+\\nu^{-1}\\sum_{n=1}^N y_n\\right)^{-1}~\\color{red}{\\tau^{-1}}\\right)\\) 第3刷の修正に関し，岩重文也氏，橋本真太郎准教授（広島大学），千葉航平助教（大阪大学），森田潤一氏(京都女子高校講師)に貴重なご意見を頂いた．この場をお借りして感謝いたします．注：所属はご指摘いただいた当時． "]]
